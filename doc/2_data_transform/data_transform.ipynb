{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Libraries & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Librarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import pkg_resources\n",
    "import tempfile\n",
    "import pprint\n",
    "import os\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "import tensorflow_data_validation as tfdv\n",
    "import apache_beam as beam\n",
    "\n",
    "\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "from tensorflow_transform.coders import example_proto_coder\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from apache_beam.io import tfrecordio\n",
    "\n",
    "print('INFO: TF version -- {}'.format(tf.__version__))\n",
    "print('INFO: TFT version -- {}'.format(tft.version.__version__))\n",
    "print('INFO: TFDV version -- {}'.format(tfdv.version.__version__))\n",
    "print('INFO: Apache Beam version -- {}'.format(beam.version.__version__))\n",
    "print('INFO: Pyarrow version -- {}'.format(pkg_resources.get_distribution(\"pyarrow\").version))\n",
    "print('INFO: TFX-BSL version -- {}'.format(pkg_resources.get_distribution(\"tfx-bsl\").version))\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of input arguments for the data validation component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"irn-70656-dev-1307100302\"\n",
    "REGION = 'europe-west1'\n",
    "RAW_DATA_PATH = \"gs://bike-sharing-data/\"\n",
    "BUCKET = \"bike-sharing-pipeline-metadata\"\n",
    "PIPELINE_VERSION = \"v0_1\"\n",
    "DATA_VERSION = \"200911_131245\"\n",
    "RUNNER = \"DirectRunner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up GCP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up some globals for the gcs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some globals for gcs file\n",
    "HANDLER = 'gs://' # ../ for local data, gs:// for cloud data\n",
    "\n",
    "RAW_TRAIN_PATH = os.path.join(RAW_DATA_PATH, \"train.csv\")\n",
    "RAW_VAL_PATH = os.path.join(RAW_DATA_PATH, \"val.csv\")\n",
    "RAW_TEST_PATH = os.path.join(RAW_DATA_PATH, \"test.csv\")\n",
    "\n",
    "BASE_DIR = os.path.join(HANDLER, BUCKET, PIPELINE_VERSION)\n",
    "RUN_DIR = os.path.join(BASE_DIR, 'run', DATA_VERSION)\n",
    "\n",
    "STAGING_DIR = os.path.join(RUN_DIR, 'staging')\n",
    "OUTPUT_DIR = os.path.join(RUN_DIR, 'data_transform')\n",
    "\n",
    "RAW_SCHEMA_PATH = RUN_DIR+'/data_validation/schema/data_schema.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HANDLER'] = HANDLER\n",
    "os.environ['OUTPUT_DIR'] = OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features, labels, and key columns\n",
    "NUMERIC_FEATURE_KEYS=[\"temp\", \"atemp\", \"humidity\", \"windspeed\"] \n",
    "CATEGORICAL_FEATURE_KEYS=[\"season\", \"weather\", \"daytype\"] \n",
    "KEY_COLUMN = \"datetime\"\n",
    "LABEL_COLUMN = \"count\"\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that allows to split dataset\n",
    "def split_dataset(row, num_partitions, ratio):\n",
    "    assert num_partitions == len(ratio)\n",
    "\n",
    "    bucket = hash(row['datetime'][0]) % sum(ratio)\n",
    "    total = 0\n",
    "    for i, part in enumerate(ratio):\n",
    "        total += part\n",
    "        if bucket < total:\n",
    "            return i\n",
    "    return len(ratio) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to scale numerical features and label encode categorical features\n",
    "def preprocessing_fn(inputs):\n",
    "      \n",
    "    outputs = {}\n",
    "    \n",
    "    for key in NUMERIC_FEATURE_KEYS:\n",
    "        outputs[transformed_name(key)] = tft.scale_to_z_score(squeeze(inputs[key]))\n",
    "    \n",
    "    for key in CATEGORICAL_FEATURE_KEYS:    \n",
    "        outputs[transformed_name(key)] = squeeze(inputs[key])\n",
    "        tft.vocabulary(inputs[key], vocab_filename=key)    \n",
    "\n",
    "    outputs[transformed_name(LABEL_COLUMN)] = squeeze(inputs[LABEL_COLUMN])\n",
    "    outputs[transformed_name(KEY_COLUMN)] = squeeze(inputs[KEY_COLUMN])\n",
    "\n",
    "    return outputs\n",
    "\n",
    "def squeeze(x):\n",
    "    return tf.squeeze(x, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ML dataset using tf.transform and DataflowÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'bike-sharing-data-transform' + '-' + datetime.now().strftime('%y%m%d-%H%M%S')    \n",
    "in_test_mode = True\n",
    "\n",
    "if RUNNER == 'DirectRunner':\n",
    "    import shutil\n",
    "    print('Launching local job ... hang on')\n",
    "    #OUTPUT_DIR = './preproc_tft'\n",
    "    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "if RUNNER == 'DataflowRunner':\n",
    "    print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
    "   # OUTPUT_DIR = 'gs://{0}/taxifare/preproc_tft/'.format(BUCKET)\n",
    "    import subprocess\n",
    "    subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
    "\n",
    "options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT,\n",
    "    'max_num_workers': 4,\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True,\n",
    "    'requirements_file': 'requirements.txt'\n",
    "}\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "# Load raw data schema and convert to tft metadata\n",
    "raw_schema = tfdv.load_schema_text(input_path=RAW_SCHEMA_PATH)\n",
    "raw_metadata = dataset_metadata.DatasetMetadata(raw_schema)\n",
    "ordered_columns = [i.name for i in raw_schema.feature]\n",
    "\n",
    "converter = tft.coders.CsvCoder(ordered_columns, raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "    with tft_beam.Context(temp_dir=tempfile.mkdtemp()):\n",
    "        \n",
    "        # Read raw train data from csv \n",
    "        raw_train_data = (p \n",
    "        | 'ReadTrainData' >> beam.io.ReadFromText(RAW_TRAIN_PATH, skip_header_lines=1)\n",
    "        | 'DecodeTrainData' >> beam.Map(converter.decode))\n",
    "                          \n",
    "        # avoid data leakage from raw_metadata! split tests before data validation\n",
    "        raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "        \n",
    "        # Analyze and transform data \n",
    "        transformed_train_dataset, transform_fn = (  \n",
    "            raw_train_dataset | \"TransformTrainData\" >> tft_beam.AnalyzeAndTransformDataset(\n",
    "                preprocessing_fn)) \n",
    "        \n",
    "        transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "    \n",
    "        # Save transformed train data to disk in efficient tfrecord format\n",
    "        transformed_train_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
    "            os.path.join(OUTPUT_DIR, 'train'), file_name_suffix='.gz',\n",
    "            coder=example_proto_coder.ExampleProtoCoder(\n",
    "                transformed_metadata.schema))\n",
    "    \n",
    "        # save transformation function to disk for use at serving time\n",
    "        transform_fn | 'WriteTransformFn' >> tft_beam.WriteTransformFn(\n",
    "            os.path.join(OUTPUT_DIR, 'tft_output'))                \n",
    "        raw_metadata | 'WriteDataMetadata' >> tft_beam.WriteMetadata(\n",
    "            os.path.join(OUTPUT_DIR, 'tft_output', 'metadata'), pipeline=p)  \n",
    "        \n",
    "        raw_val_data = (p \n",
    "        | 'ReadValData' >> beam.io.ReadFromText(RAW_VAL_PATH, skip_header_lines=1)\n",
    "        | 'DecodeValData' >> beam.Map(converter.decode))\n",
    "        \n",
    "        # avoid data leakage from raw_metadata! split tests before data validation        \n",
    "        raw_val_dataset = (raw_val_data, raw_metadata)\n",
    "\n",
    "        # Transform val data\n",
    "        transformed_val_dataset = (\n",
    "            (raw_val_dataset, transform_fn) | \"TransformValData\" >> tft_beam.TransformDataset()\n",
    "            )\n",
    "        \n",
    "        transformed_val_data, _ = transformed_val_dataset\n",
    "\n",
    "        # Save transformed train data to disk in efficient tfrecord format\n",
    "        transformed_val_data | 'WriteValData' >> tfrecordio.WriteToTFRecord(\n",
    "            os.path.join(OUTPUT_DIR, 'val'), file_name_suffix='.gz',\n",
    "            coder=example_proto_coder.ExampleProtoCoder(\n",
    "                transformed_metadata.schema))\n",
    "        \n",
    "        raw_test_data = (p \n",
    "        | 'ReadTestData' >> beam.io.ReadFromText(RAW_TEST_PATH, skip_header_lines=1)\n",
    "        | 'DecodeTestData' >> beam.Map(converter.decode))\n",
    "        \n",
    "        # avoid data leakage from raw_metadata! split tests before data validation        \n",
    "        raw_test_dataset = (raw_test_data, raw_metadata)\n",
    "        \n",
    "        # Transform test data\n",
    "        transformed_test_dataset = (\n",
    "            (raw_test_dataset, transform_fn) | \"TransformTestData\" >> tft_beam.TransformDataset()\n",
    "            )\n",
    "        \n",
    "        transformed_test_data, _ = transformed_test_dataset\n",
    "        \n",
    "        # Save transformed train data to disk in efficient tfrecord format\n",
    "        transformed_test_data | 'WriteTestData' >> tfrecordio.WriteToTFRecord(\n",
    "            os.path.join(OUTPUT_DIR, 'test'), file_name_suffix='.gz',\n",
    "            coder=example_proto_coder.ExampleProtoCoder(\n",
    "                transformed_metadata.schema))                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check output directory for train, val and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! [ \"$HANDLER\" = \"gs://\" ]; then\n",
    "    find $OUTPUT_DIR \n",
    "else\n",
    "    gsutil ls $OUTPUT_DIR\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
