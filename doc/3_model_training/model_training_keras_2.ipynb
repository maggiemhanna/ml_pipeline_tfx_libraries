{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"irn-70656-dev-1307100302\"\n",
    "REGION = 'europe-west1'\n",
    "BUCKET = \"bike-sharing-pipeline-metadata\"\n",
    "PIPELINE_VERSION = \"v0_1\"\n",
    "DATA_VERSION = \"200909_154702\"\n",
    "MODEL_VERSION = datetime.now().strftime('%y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some globals for gcs file\n",
    "HANDLER = 'gs://' # ../ for local data, gs:// for cloud data\n",
    "\n",
    "BASE_DIR = HANDLER + BUCKET+'/'+PIPELINE_VERSION\n",
    "RUN_DIR = BASE_DIR+'/run/'+DATA_VERSION\n",
    "DATA_DIR = RUN_DIR+'/data_transform'\n",
    "OUTPUT_DIR = RUN_DIR+'/model_training/' + MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Text\n",
    "\n",
    "import os\n",
    "import absl\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# Features, labels, and key columns\n",
    "NUMERIC_FEATURE_KEYS=[\"temp\", \"atemp\", \"humidity\", \"windspeed\"] \n",
    "CATEGORICAL_FEATURE_KEYS=[\"season\", \"weather\", \"daytype\"] \n",
    "KEY_COLUMN = \"datetime\"\n",
    "LABEL_COLUMN = \"count\"\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key \n",
    "\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "    return tf.data.TFRecordDataset(\n",
    "      filenames,\n",
    "      compression_type='GZIP')\n",
    "\n",
    "def get_dataset_size(file_path):\n",
    "    \"\"\"Function that fetchs the size of the Tfrecords dataset.\"\"\"\n",
    "    size = 1\n",
    "    file_list = tf.io.gfile.glob(file_path)\n",
    "    for file in file_list:\n",
    "        for record in tf.compat.v1.io.tf_record_iterator(file, options=tf.io.TFRecordOptions(\n",
    "    compression_type='GZIP')):\n",
    "            size += 1\n",
    "    return size\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, label_column, tf_transform_output):\n",
    "    \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(label_column)\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        return model(transformed_features)\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              label_column,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 16) -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "    Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "    Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    transformed_feature_spec = (\n",
    "      tf_transform_output.transformed_feature_spec().copy())\n",
    "\n",
    "    INPUT_KEYS = NUMERIC_FEATURE_KEYS + CATEGORICAL_FEATURE_KEYS + [LABEL_COLUMN]\n",
    "    transformed_feature_spec = {key: transformed_feature_spec[key] for key in INPUT_KEYS}\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "      file_pattern=file_pattern,\n",
    "      batch_size=batch_size,\n",
    "      features=transformed_feature_spec,\n",
    "      reader=_gzip_reader_fn,\n",
    "      label_key=transformed_name(label_column))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def _build_keras_model(tf_transform_output: tft.TFTransformOutput,\n",
    "                       hidden_units: List[int] = None,\n",
    "                       learning_rate: float = 0.01) -> tf.keras.Model:\n",
    "    \"\"\"Creates a DNN Keras model for classifying taxi data.\n",
    "\n",
    "    Args:\n",
    "    hidden_units: [int], the layer sizes of the DNN (input layer first).\n",
    "\n",
    "    Returns:\n",
    "    A keras Model.\n",
    "    \"\"\"\n",
    "    numeric_columns = [\n",
    "      tf.feature_column.numeric_column(transformed_name(key), shape=())\n",
    "      for key in NUMERIC_FEATURE_KEYS\n",
    "    ]\n",
    "    \n",
    "    categorical_columns = [\n",
    "      tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "        transformed_name(key), \n",
    "        vocabulary_file=tf_transform_output.vocabulary_file_by_name(\n",
    "            vocab_filename=key), \n",
    "        dtype=tf.dtypes.string,\n",
    "        default_value=None, \n",
    "        num_oov_buckets=0)\n",
    "      for key in CATEGORICAL_FEATURE_KEYS\n",
    "    ]\n",
    "    \n",
    "    indicator_columns = [\n",
    "      tf.feature_column.indicator_column(categorical_column)\n",
    "      for categorical_column in categorical_columns\n",
    "    ]\n",
    "        \n",
    "    model = dnn_regressor(\n",
    "      input_columns=numeric_columns + indicator_columns,\n",
    "      dnn_hidden_units=hidden_units,\n",
    "      learning_rate=learning_rate)\n",
    "    return model\n",
    "\n",
    "\n",
    "def dnn_regressor(input_columns, dnn_hidden_units, learning_rate):\n",
    "    \"\"\"Build a simple keras wide and deep model.\n",
    "\n",
    "    Args:\n",
    "    wide_columns: Feature columns wrapped in indicator_column for wide (linear)\n",
    "      part of the model.\n",
    "    deep_columns: Feature columns for deep part of the model.\n",
    "    dnn_hidden_units: [int], the layer sizes of the hidden DNN.\n",
    "\n",
    "    Returns:\n",
    "    A Wide and Deep Keras model\n",
    "    \"\"\"\n",
    "    # Following values are hard coded for simplicity in this example,\n",
    "    # However prefarably they should be passsed in as hparams.\n",
    "\n",
    "\n",
    "    input_layers = {\n",
    "      colname: tf.keras.layers.Input(name=transformed_name(colname), shape=(), dtype=tf.float32)\n",
    "      for colname in NUMERIC_FEATURE_KEYS\n",
    "    }\n",
    "    input_layers.update({\n",
    "      colname: tf.keras.layers.Input(name=transformed_name(colname), shape=(), dtype='string')\n",
    "      for colname in CATEGORICAL_FEATURE_KEYS\n",
    "    })\n",
    "\n",
    "    deep = tf.keras.layers.DenseFeatures(input_columns)(input_layers)\n",
    "    for numnodes in dnn_hidden_units:\n",
    "        deep = tf.keras.layers.Dense(numnodes, activation='relu')(deep)\n",
    "\n",
    "    output = tf.keras.layers.Dense(\n",
    "      1, activation=None)(deep)\n",
    "\n",
    "    model = tf.keras.Model(input_layers, output)\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss = \"mean_squared_error\",\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "    model.summary(print_fn=absl.logging.info)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(params):\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "    params: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO) # so loss is printed during training\n",
    "    \n",
    "    # Extract params from task.py\n",
    "    DATA_DIR = params[\"data_dir\"]\n",
    "    OUTPUT_DIR = params[\"output_dir\"]\n",
    "    HIDDEN_UNITS_1 = params[\"hidden_units_1\"]\n",
    "    HIDDEN_UNITS_2 = params[\"hidden_units_2\"]\n",
    "    HIDDEN_UNITS_3 = params[\"hidden_units_3\"]\n",
    "    BATCH_SIZE = params[\"batch_size\"]\n",
    "    NUM_EPOCHS = params[\"num_epochs\"]\n",
    "    LEARNING_RATE = params[\"learning_rate\"]\n",
    "    \n",
    "    # Setting up paths \n",
    "    TRAIN_PATH = DATA_DIR+'/train*'\n",
    "    VAL_PATH = DATA_DIR+'/val*'\n",
    "    TEST_PATH = DATA_DIR+'/test*'\n",
    "    \n",
    "    # Training set size\n",
    "    TRAIN_SIZE = get_dataset_size(TRAIN_PATH)\n",
    "    NUM_STEPS = TRAIN_SIZE / BATCH_SIZE # number of steps per epoch for which to train model\n",
    "    \n",
    "    tf_transform_output = tft.TFTransformOutput(os.path.join(DATA_DIR, 'tft_output'))\n",
    "        \n",
    "    train_dataset = _input_fn(TRAIN_PATH, LABEL_COLUMN, tf_transform_output, BATCH_SIZE)\n",
    "    val_dataset = _input_fn(VAL_PATH, LABEL_COLUMN, tf_transform_output, BATCH_SIZE)\n",
    "\n",
    "    model = _build_keras_model(\n",
    "        tf_transform_output,\n",
    "        hidden_units=[HIDDEN_UNITS_1, HIDDEN_UNITS_2, HIDDEN_UNITS_3],\n",
    "        learning_rate=LEARNING_RATE)\n",
    "\n",
    "    log_dir = os.path.join(OUTPUT_DIR, 'logs')\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # Setup Metric callback.\n",
    "    class metric_cb(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            tf.summary.scalar('rmse', logs['rmse'], epoch)    \n",
    "    \n",
    "    model.fit(\n",
    "      train_dataset,\n",
    "      epochs=NUM_EPOCHS,        \n",
    "      steps_per_epoch=NUM_STEPS,\n",
    "      validation_data=val_dataset,\n",
    "      validation_steps=10,\n",
    "      callbacks=[tensorboard_callback, metric_cb()])\n",
    "\n",
    "    signatures = {\n",
    "      'serving_default':\n",
    "          _get_serve_tf_examples_fn(model,\n",
    "                                    LABEL_COLUMN,\n",
    "                                    tf_transform_output).get_concrete_function(\n",
    "                                        tf.TensorSpec(\n",
    "                                            shape=[None],\n",
    "                                            dtype=tf.string,\n",
    "                                            name='examples')),\n",
    "    }\n",
    "    model.save(OUTPUT_DIR, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"data_dir\": DATA_DIR,\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"hidden_units_1\": 16,\n",
    "    \"hidden_units_2\": 32,\n",
    "    \"hidden_units_3\": 64,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 0.0001,\n",
    "\n",
    "}\n",
    "train_and_evaluate(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
