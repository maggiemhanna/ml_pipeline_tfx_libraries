{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Libraries & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of input arguments for the data validation component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"irn-70656-dev-1307100302\"\n",
    "REGION = 'europe-west1'\n",
    "BUCKET = \"bike-sharing-pipeline-metadata\"\n",
    "BUCKET_STAGING = \"bike-sharing-pipeline-staging\"\n",
    "PIPELINE_VERSION = \"v0_1\"\n",
    "DATA_VERSION = \"200909_154702\"\n",
    "MODEL_VERSION = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "RUNNER = \"AIplatformRunner\" # DirectRunner or AIplatformRunner to run on AI platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_VERSION = '2.2'\n",
    "PYTHON_VERSION = '3.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some globals for gcs file\n",
    "HANDLER = 'gs://' # ../ for local data, gs:// for cloud data\n",
    "\n",
    "BASE_DIR = HANDLER + BUCKET+'/'+PIPELINE_VERSION\n",
    "RUN_DIR = BASE_DIR+'/run/'+DATA_VERSION\n",
    "DATA_DIR = RUN_DIR+'/data_transform'\n",
    "OUTPUT_DIR = RUN_DIR+'/model_training/' + MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['BUCKET_STAGING'] = BUCKET_STAGING\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['PIPELINE_VERSION'] = PIPELINE_VERSION\n",
    "os.environ['MODEL_VERSION'] = MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HANDLER'] = HANDLER\n",
    "os.environ['RUNNER'] = RUNNER\n",
    "os.environ['TF_VERSION'] = TF_VERSION\n",
    "os.environ['PYTHON_VERSION'] = PYTHON_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DATA_DIR'] = DATA_DIR\n",
    "os.environ['OUTPUT_DIR'] = OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If running on google cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up project and compute region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move code into python package\n",
    "\n",
    "Let's package our updated code with feature engineering so it's AI Platform compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir bikesharingmodel\n",
    "mkdir bikesharingmodel/trainer\n",
    "touch bikesharingmodel/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bikesharingmodel/PKG-INFO\n",
    "Metadata-Version: 1.0\n",
    "Name: trainer\n",
    "Version: 0.1\n",
    "Summary: BikeSharingModel\n",
    "Home-page: UNKNOWN\n",
    "Author: MA MHANNA\n",
    "Author-email: maggie.mhanna@renault.com\n",
    "License: Apache\n",
    "Description: UNKNOWN\n",
    "Platform: UNKNOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bikesharingmodel/setup.cfg\n",
    "[egg_info]\n",
    "tag_build = \n",
    "tag_date = 0\n",
    "tag_svn_revision = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bikesharingmodel/setup.py\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "NAME = 'trainer'\n",
    "VERSION = '0.1'\n",
    "AUTHOR = 'MA MHANNA'\n",
    "EMAIL = 'maggie.mhanna@renault.com'\n",
    "\n",
    "REQUIRED_PACKAGES = ['tensorflow==2.2.0', \n",
    "                     'tensorflow-cpu==2.2.0',\n",
    "                     'tensorflow_transform==0.22.0',\n",
    "                     'tensorflow_model_analysis==0.22.0', \n",
    "                     'apache_beam[gcp]==2.20.0', \n",
    "                     'pyarrow==0.16.0',\n",
    "                     'tfx-bsl==0.22.0',\n",
    "                     'absl-py==0.8.1']\n",
    "\n",
    "\n",
    "setup(\n",
    "    name=NAME,\n",
    "    version=VERSION,\n",
    "    author = AUTHOR,\n",
    "    author_email = EMAIL,\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Bike Sharing Demand Prediction.',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bikesharingmodel/model.py\n",
    "\n",
    "from typing import List, Text\n",
    "\n",
    "import os\n",
    "import absl\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "# Features, labels, and key columns\n",
    "NUMERIC_FEATURE_KEYS=[\"temp\", \"atemp\", \"humidity\", \"windspeed\"] \n",
    "CATEGORICAL_FEATURE_KEYS=[\"season\", \"weather\", \"daytype\"] \n",
    "KEY_COLUMN = \"datetime\"\n",
    "LABEL_COLUMN = \"count\"\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key \n",
    "\n",
    "\n",
    "def _gzip_reader_fn(filenames):\n",
    "    \"\"\"Small utility returning a record reader that can read gzip'ed files.\"\"\"\n",
    "    return tf.data.TFRecordDataset(\n",
    "      filenames,\n",
    "      compression_type='GZIP')\n",
    "\n",
    "def get_dataset_size(file_path):\n",
    "    \"\"\"Function that fetchs the size of the Tfrecords dataset.\"\"\"\n",
    "    size = 1\n",
    "    file_list = tf.io.gfile.glob(file_path)\n",
    "    for file in file_list:\n",
    "        for record in tf.compat.v1.io.tf_record_iterator(file, options=tf.io.TFRecordOptions(\n",
    "    compression_type='GZIP')):\n",
    "            size += 1\n",
    "    return size\n",
    "\n",
    "def _get_serve_tf_examples_fn(model, label_column, tf_transform_output):\n",
    "    \"\"\"Returns a function that parses a serialized tf.Example and applies TFT.\"\"\"\n",
    "\n",
    "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "    @tf.function\n",
    "    def serve_tf_examples_fn(serialized_tf_examples):\n",
    "        \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
    "        feature_spec = tf_transform_output.raw_feature_spec()\n",
    "        feature_spec.pop(label_column)\n",
    "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
    "        transformed_features = model.tft_layer(parsed_features)\n",
    "        return model(transformed_features)\n",
    "\n",
    "    return serve_tf_examples_fn\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              label_column,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 16) -> tf.data.Dataset:\n",
    "    \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "    Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "    Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    transformed_feature_spec = (\n",
    "      tf_transform_output.transformed_feature_spec().copy())\n",
    "\n",
    "    INPUT_KEYS = NUMERIC_FEATURE_KEYS + CATEGORICAL_FEATURE_KEYS + [LABEL_COLUMN]\n",
    "    transformed_feature_spec = {key: transformed_feature_spec[key] for key in INPUT_KEYS}\n",
    "\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "      file_pattern=file_pattern,\n",
    "      batch_size=batch_size,\n",
    "      features=transformed_feature_spec,\n",
    "      reader=_gzip_reader_fn,\n",
    "      label_key=transformed_name(label_column))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Evaluation Metrics\n",
    "def rmse(labels, predictions): # Root Mean Squared Error\n",
    "    rmse = tf.sqrt(x = tf.reduce_mean(input_tensor = tf.square(x = labels - predictions)))\n",
    "    return  rmse\n",
    "\n",
    "def _build_keras_model(tf_transform_output: tft.TFTransformOutput,\n",
    "                       hidden_units: List[int] = None,\n",
    "                       learning_rate: float = 0.01) -> tf.keras.Model:\n",
    "    \"\"\"Creates a DNN Keras model for classifying taxi data.\n",
    "\n",
    "    Args:\n",
    "    hidden_units: [int], the layer sizes of the DNN (input layer first).\n",
    "\n",
    "    Returns:\n",
    "    A keras Model.\n",
    "    \"\"\"\n",
    "    numeric_columns = [\n",
    "      tf.feature_column.numeric_column(transformed_name(key), shape=())\n",
    "      for key in NUMERIC_FEATURE_KEYS\n",
    "    ]\n",
    "    \n",
    "    categorical_columns = [\n",
    "      tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "        transformed_name(key), \n",
    "        vocabulary_file=tf_transform_output.vocabulary_file_by_name(\n",
    "            vocab_filename=key), \n",
    "        dtype=tf.dtypes.string,\n",
    "        default_value=None, \n",
    "        num_oov_buckets=0)\n",
    "      for key in CATEGORICAL_FEATURE_KEYS\n",
    "    ]\n",
    "    \n",
    "    indicator_columns = [\n",
    "      tf.feature_column.indicator_column(categorical_column)\n",
    "      for categorical_column in categorical_columns\n",
    "    ]\n",
    "        \n",
    "    model = dnn_regressor(\n",
    "      input_columns=numeric_columns + indicator_columns,\n",
    "      dnn_hidden_units=hidden_units,\n",
    "      learning_rate=learning_rate)\n",
    "    return model\n",
    "\n",
    "\n",
    "def dnn_regressor(input_columns, dnn_hidden_units, learning_rate):\n",
    "    \"\"\"Build a simple keras wide and deep model.\n",
    "\n",
    "    Args:\n",
    "    wide_columns: Feature columns wrapped in indicator_column for wide (linear)\n",
    "      part of the model.\n",
    "    deep_columns: Feature columns for deep part of the model.\n",
    "    dnn_hidden_units: [int], the layer sizes of the hidden DNN.\n",
    "\n",
    "    Returns:\n",
    "    A Wide and Deep Keras model\n",
    "    \"\"\"\n",
    "    # Following values are hard coded for simplicity in this example,\n",
    "    # However prefarably they should be passsed in as hparams.\n",
    "\n",
    "\n",
    "    input_layers = {\n",
    "      colname: tf.keras.layers.Input(name=transformed_name(colname), shape=(), dtype=tf.float32)\n",
    "      for colname in NUMERIC_FEATURE_KEYS\n",
    "    }\n",
    "    input_layers.update({\n",
    "      colname: tf.keras.layers.Input(name=transformed_name(colname), shape=(), dtype='string')\n",
    "      for colname in CATEGORICAL_FEATURE_KEYS\n",
    "    })\n",
    "\n",
    "    deep = tf.keras.layers.DenseFeatures(input_columns)(input_layers)\n",
    "    for numnodes in dnn_hidden_units:\n",
    "        deep = tf.keras.layers.Dense(numnodes, activation='relu')(deep)\n",
    "\n",
    "    output = tf.keras.layers.Dense(\n",
    "      1, activation=None)(deep)\n",
    "\n",
    "    model = tf.keras.Model(input_layers, output)\n",
    "    model.compile(\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=learning_rate),\n",
    "        loss = \"mean_squared_error\",\n",
    "        metrics=[rmse])\n",
    "    model.summary(print_fn=absl.logging.info)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_evaluate(params):\n",
    "    \"\"\"Train the model based on given args.\n",
    "\n",
    "    Args:\n",
    "    params: Holds args used to train the model as name/value pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO) # so loss is printed during training\n",
    "    \n",
    "    # Extract params from task.py\n",
    "    DATA_DIR = params[\"data_dir\"]\n",
    "    OUTPUT_DIR = params[\"output_dir\"]\n",
    "    HIDDEN_UNITS_1 = params[\"hidden_units_1\"]\n",
    "    HIDDEN_UNITS_2 = params[\"hidden_units_2\"]\n",
    "    HIDDEN_UNITS_3 = params[\"hidden_units_3\"]\n",
    "    BATCH_SIZE = params[\"batch_size\"]\n",
    "    NUM_EPOCHS = params[\"num_epochs\"]\n",
    "    LEARNING_RATE = params[\"learning_rate\"]\n",
    "    \n",
    "    # Setting up paths \n",
    "    TRAIN_PATH = DATA_DIR+'/train*'\n",
    "    VAL_PATH = DATA_DIR+'/val*'\n",
    "    TEST_PATH = DATA_DIR+'/test*'\n",
    "    \n",
    "    # Training set size\n",
    "    TRAIN_SIZE = get_dataset_size(TRAIN_PATH)\n",
    "    NUM_STEPS = TRAIN_SIZE / BATCH_SIZE # number of steps per epoch for which to train model\n",
    "    \n",
    "    tf_transform_output = tft.TFTransformOutput(os.path.join(DATA_DIR, 'tft_output'))\n",
    "        \n",
    "    train_dataset = _input_fn(TRAIN_PATH, LABEL_COLUMN, tf_transform_output, BATCH_SIZE)\n",
    "    val_dataset = _input_fn(VAL_PATH, LABEL_COLUMN, tf_transform_output, BATCH_SIZE)\n",
    "\n",
    "    model = _build_keras_model(\n",
    "        tf_transform_output,\n",
    "        hidden_units=[HIDDEN_UNITS_1, HIDDEN_UNITS_2, HIDDEN_UNITS_3],\n",
    "        learning_rate=LEARNING_RATE)\n",
    "\n",
    "    log_dir = os.path.join(OUTPUT_DIR, 'logs')\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir, histogram_freq=1)\n",
    "    \n",
    "    # Setup Metric callback.\n",
    "    class metric_cb(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            tf.summary.scalar('rmse', logs['rmse'], epoch) \n",
    "    \n",
    "    model.fit(\n",
    "      train_dataset,\n",
    "      epochs=NUM_EPOCHS,        \n",
    "      steps_per_epoch=NUM_STEPS,\n",
    "      validation_data=val_dataset,\n",
    "      validation_steps=10,\n",
    "      callbacks=[tensorboard_callback, metric_cb()])\n",
    "\n",
    "    signatures = {\n",
    "      'serving_default':\n",
    "          _get_serve_tf_examples_fn(model,\n",
    "                                    LABEL_COLUMN,\n",
    "                                    tf_transform_output).get_concrete_function(\n",
    "                                        tf.TensorSpec(\n",
    "                                            shape=[None],\n",
    "                                            dtype=tf.string,\n",
    "                                            name='examples')),\n",
    "    }\n",
    "    model.save(OUTPUT_DIR, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bikesharingmodel/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        help = \"Location directory to read datasets\",\n",
    "        type = str,\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help = \"Location directory to write checkpoints and export models\",\n",
    "        type = str,\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_units_1\",\n",
    "        help = \"Hidden layer 1 sizes to use for DNN feature columns\",\n",
    "        type = int,\n",
    "        default = 16\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_units_2\",\n",
    "        help = \"Hidden layer 2 sizes to use for DNN feature columns\",\n",
    "        type = int,\n",
    "        default = 16\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_units_3\",\n",
    "        help = \"Hidden layer 3 sizes to use for DNN feature columns\",\n",
    "        type = int,\n",
    "        default = 16\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help = \"Mini-batch gradient descent batch size\",\n",
    "        type = int,\n",
    "        default = 16\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help = \"Number of epochs\",\n",
    "        type = int,\n",
    "        default = 10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        help = \"Hidden layer 1 sizes to use for DNN feature columns\",\n",
    "        type = int,\n",
    "        default = 0.001\n",
    "    )\n",
    "    args = parser.parse_args().__dict__\n",
    "    \n",
    "    # Append trial_id to path so trials don\"t overwrite each other\n",
    "    args[\"output_dir\"] = os.path.join(\n",
    "        args[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    ) \n",
    "        \n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"DirectRunner\" ]; then\n",
    "\n",
    "    # Start fresh each time\n",
    "    if ! [ \"$HANDLER\" = \"gs://\" ]; then\n",
    "        rm -R ${OUTPUT_DIR}\n",
    "    else\n",
    "        gsutil -m rm -R ${OUTPUT_DIR}\n",
    "    fi  \n",
    "\n",
    "    python -m bikesharingmodel.task \\\n",
    "        --data_dir=${DATA_DIR} \\\n",
    "        --output_dir=${OUTPUT_DIR}\n",
    "else \n",
    "echo \"RUNNER = \" $RUNNER\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running locally using gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"DirectRunner\" ]; then\n",
    "\n",
    "    # Start fresh each time\n",
    "    if ! [ \"$HANDLER\" = \"gs://\" ]; then\n",
    "        rm -R ${OUTPUT_DIR}\n",
    "    else\n",
    "        gsutil -m rm -R ${OUTPUT_DIR}\n",
    "    fi \n",
    "\n",
    "    # Use AI Platform to train the model in local file system\n",
    "    gcloud ai-platform local train \\\n",
    "        --module-name=bikesharingmodel.task \\\n",
    "        --package-path=bikesharingmodel \\\n",
    "        -- \\\n",
    "        --data_dir=${DATA_DIR} \\\n",
    "        --output_dir=${OUTPUT_DIR}\n",
    "else \n",
    "echo \"RUNNER = \" $RUNNER\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on AI Platform using gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: n1-standard-4\n",
    "  evaluatorType: n1-standard-4\n",
    "  evaluatorCount: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"AIplatformRunner\" ]; then\n",
    "\n",
    "    # Start fresh each time\n",
    "    if ! [ \"$HANDLER\" = \"gs://\" ]; then\n",
    "        rm -R ${OUTPUT_DIR}\n",
    "    else\n",
    "        gsutil -m rm -R ${OUTPUT_DIR}\n",
    "    fi \n",
    "\n",
    "    JOB_NAME=${PIPELINE_VERSION}_${MODEL_VERSION}\n",
    "    echo $OUTPUT_DIR $REGION $JOB_NAME\n",
    "\n",
    "    gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "       --region=$REGION \\\n",
    "       --module-name=bikesharingmodel.task \\\n",
    "       --package-path=bikesharingmodel \\\n",
    "       --staging-bucket=gs://$BUCKET_STAGING \\\n",
    "       --config=config.yaml \\\n",
    "       --runtime-version=$TF_VERSION \\\n",
    "       --python-version=$PYTHON_VERSION \\\n",
    "       -- \\\n",
    "       --data_dir=$DATA_DIR \\\n",
    "       --output_dir=$OUTPUT_DIR \n",
    "else \n",
    "echo \"RUNNER = \" $RUNNER\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"AIplatformRunner\" ]; then\n",
    "    JOB_NAME=${PIPELINE_VERSION}_${MODEL_VERSION}\n",
    "    gcloud ai-platform jobs describe $JOB_NAME\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on AI Platform with Hyperparameter Tuning\n",
    "\n",
    "AI Platform HyperTune, powered by Google Vizier, uses Bayesian Optimization by default, but also supports Grid Search and Random Search.\n",
    "\n",
    "When tuning just a few hyperparameters (say less than 4), Grid Search and Random Search work well, but when tunining several hyperparameters and the search space is large Bayesian Optimization is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: n1-standard-4\n",
    "  evaluatorType: n1-standard-4\n",
    "  evaluatorCount: 1\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 2\n",
    "    maxParallelTrials: 2\n",
    "    hyperparameterMetricTag: rmse\n",
    "    params:\n",
    "    - parameterName: hidden_units_1\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: hidden_units_2\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: hidden_units_3\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: num_epochs\n",
    "      type: INTEGER\n",
    "      minValue: 1\n",
    "      maxValue: 20\n",
    "      scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"AIplatformRunner\" ]; then\n",
    "\n",
    "    # Start fresh each time\n",
    "    if ! [ \"$HANDLER\" = \"gs://\" ]; then\n",
    "        rm -R ${OUTPUT_DIR}\n",
    "    else\n",
    "        gsutil -m rm -R ${OUTPUT_DIR}\n",
    "    fi \n",
    "\n",
    "    JOB_NAME=${PIPELINE_VERSION}_${MODEL_VERSION}_hypertune\n",
    "    echo $OUTPUT_DIR $REGION $JOBNAME\n",
    "\n",
    "    gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "       --region=$REGION \\\n",
    "       --module-name=bikesharingmodel.task \\\n",
    "       --package-path=bikesharingmodel \\\n",
    "       --staging-bucket=gs://$BUCKET_STAGING \\\n",
    "       --runtime-version=$TF_VERSION \\\n",
    "       --python-version=$PYTHON_VERSION \\\n",
    "       --config=hyperparam.yaml \\\n",
    "       -- \\\n",
    "       --data_dir=$DATA_DIR \\\n",
    "       --output_dir=$OUTPUT_DIR  \n",
    "else \n",
    "echo \"RUNNER = \" $RUNNER\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"AIplatformRunner\" ]; then\n",
    "    JOB_NAME=${PIPELINE_VERSION}_${MODEL_VERSION}_hypertune\n",
    "    gcloud ai-platform jobs describe $JOB_NAME\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
