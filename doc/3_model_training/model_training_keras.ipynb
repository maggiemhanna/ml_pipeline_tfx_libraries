{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Libraries & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Librarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pkg_resources\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "print('INFO: TF version -- {}'.format(tf.__version__))\n",
    "print('INFO: TFT version -- {}'.format(pkg_resources.get_distribution(\"tensorflow_transform\").version))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of input arguments for the data validation component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"irn-70656-dev-1307100302\"\n",
    "REGION = 'europe-west1'\n",
    "BUCKET = \"bike-sharing-pipeline-metadata\"\n",
    "PIPELINE_VERSION = \"v0_1\"\n",
    "DATA_VERSION = \"200909_154702\"\n",
    "MODEL_VERSION = datetime.now().strftime('%y%m%d_%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features, labels, and key columns\n",
    "NUMERIC_FEATURE_KEYS=[\"temp\", \"atemp\", \"humidity\", \"windspeed\"] \n",
    "CATEGORICAL_FEATURE_KEYS=[\"season\", \"weather\", \"daytype\"] \n",
    "KEY_COLUMN = \"datetime\"\n",
    "LABEL_COLUMN = \"count\"\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up some globals for the gcs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some globals for gcs file\n",
    "HANDLER = 'gs://' # ../ for local data, gs:// for cloud data\n",
    "\n",
    "BASE_DIR = HANDLER + BUCKET+'/'+PIPELINE_VERSION\n",
    "RUN_DIR = BASE_DIR+'/run/'+DATA_VERSION\n",
    "DATA_DIR = RUN_DIR+'/data_transform'\n",
    "OUTPUT_DIR = RUN_DIR+'/model_training/' + MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['PIPELINE_VERSION'] = PIPELINE_VERSION\n",
    "os.environ['DATA_DIR'] = DATA_DIR\n",
    "os.environ['OUTPUT_DIR'] = OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up GCP project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_path, label_column, tf_transform_output, batch_size, mode = tf.estimator.ModeKeys.TRAIN):\n",
    "    \"\"\"Create an input function reading TFRecord files using the data API.\n",
    "    Args:\n",
    "        data_path: path of the data in tfrecords format\n",
    "        mode: tf estimator mode key\n",
    "        batch_size: number of observations in batch\n",
    "\n",
    "    Returns:\n",
    "        input_fn: data input function\n",
    "    \"\"\"\n",
    "    \n",
    "    features_spec = tf_transform_output.transformed_feature_spec()\n",
    "\n",
    "    def _input_fn():\n",
    "        # Create list of files in the data path\n",
    "        file_list = tf.io.gfile.glob(data_path)\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TFRecordDataset(filenames=file_list, compression_type = \"GZIP\", num_parallel_reads=5)\n",
    "        def parse_example(example):\n",
    "            parsed_features = tf.io.parse_single_example(example, features_spec)\n",
    "            label = parsed_features.pop(label_column)\n",
    "            return parsed_features, label\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely, we'll set this in train spec\n",
    "            dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after one epoch\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.map(parse_example, num_parallel_calls=5)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=1)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Evaluate input functions\n",
    "def create_feature_columns(tf_transform_output):\n",
    "    \n",
    "    numeric_columns = [\n",
    "      tf.feature_column.numeric_column(transformed_name(key))\n",
    "      for key in NUMERIC_FEATURE_KEYS\n",
    "    ]\n",
    "    \n",
    "    categorical_columns = [\n",
    "      tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "        transformed_name(key), \n",
    "        vocabulary_file=tf_transform_output.vocabulary_file_by_name(\n",
    "            vocab_filename=key), \n",
    "        dtype=tf.dtypes.string,\n",
    "        default_value=None, \n",
    "        num_oov_buckets=0)\n",
    "      for key in CATEGORICAL_FEATURE_KEYS\n",
    "    ]\n",
    "    \n",
    "    indicator_columns = [\n",
    "      tf.feature_column.indicator_column(categorical_column)\n",
    "      for categorical_column in categorical_columns\n",
    "    ]    \n",
    "    \n",
    "    feature_columns = numeric_columns + indicator_columns\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Custom Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "def rmse(labels, predictions): # Root Mean Squared Error\n",
    "    rmse = tf.sqrt(x = tf.reduce_mean(input_tensor = tf.square(x = labels - predictions)))\n",
    "    return  rmse\n",
    "\n",
    "def mae(labels, predictions): # Root Mean Squared Erro\n",
    "    mae = tf.reduce_mean(input_tensor = tf.abs(x = labels - predictions))\n",
    "    return mae \n",
    "\n",
    "# Build Custom Keras Model\n",
    "def create_keras_model(features_columns, hidden_units_1, hidden_units_2, hidden_units_3, learning_rate):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.DenseFeatures(feature_columns=features_columns, name=\"input\"))\n",
    "    model.add(tf.keras.layers.Dense(units = hidden_units_1, activation = \"relu\", name = \"dense1\"))\n",
    "    model.add(tf.keras.layers.Dense(units = hidden_units_2, activation = \"relu\", name = \"dense2\"))\n",
    "    model.add(tf.keras.layers.Dense(units = hidden_units_3, activation = \"relu\", name = \"dense3\"))\n",
    "    model.add(tf.keras.layers.Dense(units = 1, activation = None, name = \"output\"))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate),\n",
    "        loss = \"mean_squared_error\",\n",
    "        metrics = [rmse, mae])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving input function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Evaluate\n",
    "\n",
    "To train our model, we can use train_and_evaluate. Note that we use tf.keras.estimator.model_to_estimator to create our estimator. It takes as arguments the compiled keras model, the OUTDIR, and optionally a tf.estimator.Runconfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_size(file_path):\n",
    "    \"\"\"Function that fetchs the size of the Tfrecords dataset.\"\"\"\n",
    "    size = 1\n",
    "    file_list = tf.io.gfile.glob(file_path)\n",
    "    for file in file_list:\n",
    "        for record in tf.compat.v1.io.tf_record_iterator(file, options=tf.io.TFRecordOptions(\n",
    "    compression_type='GZIP')):\n",
    "            size += 1\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(params):\n",
    "    \n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO) # so loss is printed during training\n",
    "    \n",
    "    # Extract params from task.py\n",
    "    DATA_DIR = params[\"data_dir\"]\n",
    "    OUTPUT_DIR = params[\"output_dir\"]\n",
    "    HIDDEN_UNITS_1 = params[\"hidden_units_1\"]\n",
    "    HIDDEN_UNITS_2 = params[\"hidden_units_2\"]\n",
    "    HIDDEN_UNITS_3 = params[\"hidden_units_3\"]\n",
    "    BATCH_SIZE = params[\"batch_size\"]\n",
    "    NUM_EPOCHS = params[\"num_epochs\"]\n",
    "    LEARNING_RATE = params[\"learning_rate\"]\n",
    "    \n",
    "    # Setting up paths \n",
    "    TRAIN_PATH = DATA_DIR+'/train*'\n",
    "    VAL_PATH = DATA_DIR+'/val*'\n",
    "    TEST_PATH = DATA_DIR+'/test*'\n",
    "\n",
    "    # Define key and label columns\n",
    "    KEY_COLUMN = 'datetime'\n",
    "    LABEL_COLUMN = 'count'\n",
    "    \n",
    "    # Training set size\n",
    "    TRAIN_SIZE = get_dataset_size(TRAIN_PATH)\n",
    "\n",
    "    NUM_STEPS = TRAIN_SIZE / BATCH_SIZE * NUM_EPOCHS # total steps for which to train model\n",
    "    CHECKPOINTS_STEPS = 20 # checkpoint every N steps\n",
    "\n",
    "    # number of feature columns for keras input layer\n",
    "    \n",
    "    tf_transform_output = tft.TFTransformOutput(os.path.join(DATA_DIR, 'tft_output'))\n",
    "\n",
    "    features_columns = create_feature_columns(tf_transform_output)\n",
    "    \n",
    "    keras_model = create_keras_model(features_columns, HIDDEN_UNITS_1, HIDDEN_UNITS_2, \n",
    "                                     HIDDEN_UNITS_3, LEARNING_RATE)\n",
    "\n",
    "    # Setup TensorBoard callback.\n",
    "    log_dir = os.path.join(OUTPUT_DIR, 'logs')\n",
    "    tensorboard_cb = tf.keras.callbacks.TensorBoard(\n",
    "      log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "    # Setup Metric callback.\n",
    "    class metric_cb(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            tf.summary.scalar('rmse', logs['rmse'], epoch)\n",
    "            \n",
    "    train_data = input_fn(TRAIN_PATH, LABEL_COLUMN, tf_transform_output, BATCH_SIZE, tf.estimator.ModeKeys.TRAIN)()\n",
    "    val_data = input_fn(VAL_PATH, LABEL_COLUMN, tf_transform_output, BATCH_SIZE, tf.estimator.ModeKeys.EVAL)()\n",
    "    \n",
    "    # Train keras model\n",
    "    keras_model.fit(\n",
    "        train_data,\n",
    "        steps_per_epoch=NUM_STEPS,\n",
    "        epochs=NUM_EPOCHS,\n",
    "        validation_data=val_data,\n",
    "        validation_steps=5,\n",
    "        verbose=1,\n",
    "    callbacks=[tensorboard_cb, metric_cb()])\n",
    "\n",
    "    export_path = os.path.join(OUTPUT_DIR, 'export')\n",
    "    tf.keras.models.save_model(keras_model, export_path, overwrite = True, save_format=\"tf\")\n",
    "    print('Model exported to: {}'.format(export_path))\n",
    "    \n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"data_dir\": DATA_DIR,\n",
    "    \"output_dir\": OUTPUT_DIR,\n",
    "    \"hidden_units_1\": 16,\n",
    "    \"hidden_units_2\": 32,\n",
    "    \"hidden_units_3\": 64,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_epochs\": 5,\n",
    "    \"learning_rate\": 0.0001,\n",
    "\n",
    "}\n",
    "model = train_and_evaluate(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir $OUTPUT_DIR"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
