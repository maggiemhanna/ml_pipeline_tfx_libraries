{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Libraries & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of input arguments for the data validation component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"irn-70656-dev-1307100302\"\n",
    "REGION = 'europe-west1'\n",
    "BUCKET = \"bike-sharing-pipeline-metadata\"\n",
    "BUCKET_STAGING = \"bike-sharing-pipeline-staging\"\n",
    "PIPELINE_VERSION = \"v0_1\"\n",
    "DATA_VERSION = \"200909_154702\"\n",
    "MODEL_VERSION = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "RUNNER = \"AIplatformRunner\" # DirectRunner or AIplatformRunner to run on AI platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_VERSION = '2.2'\n",
    "PYTHON_VERSION = '3.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some globals for gcs file\n",
    "HANDLER = 'gs://' # ../ for local data, gs:// for cloud data\n",
    "\n",
    "BASE_DIR = os.path.join(HANDLER, BUCKET, PIPELINE_VERSION)\n",
    "RUN_DIR = os.path.join(BASE_DIR, 'run', DATA_VERSION)\n",
    "\n",
    "DATA_DIR = os.path.join(RUN_DIR, 'data_transform')\n",
    "OUTPUT_DIR = os.path.join(RUN_DIR, 'model_training', MODEL_VERSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['BUCKET_STAGING'] = BUCKET_STAGING\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['PIPELINE_VERSION'] = PIPELINE_VERSION\n",
    "os.environ['MODEL_VERSION'] = MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HANDLER'] = HANDLER\n",
    "os.environ['RUNNER'] = RUNNER\n",
    "os.environ['TF_VERSION'] = TF_VERSION\n",
    "os.environ['PYTHON_VERSION'] = PYTHON_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DATA_DIR'] = DATA_DIR\n",
    "os.environ['OUTPUT_DIR'] = OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If running on google cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up project and compute region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move code into python package\n",
    "\n",
    "Let's package our updated code with feature engineering so it's AI Platform compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir bikesharingmodel\n",
    "mkdir bikesharingmodel/trainer\n",
    "touch bikesharingmodel/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bikesharingmodel/PKG-INFO\n",
    "Metadata-Version: 1.0\n",
    "Name: trainer\n",
    "Version: 0.1\n",
    "Summary: BikeSharingModel\n",
    "Home-page: UNKNOWN\n",
    "Author: MA MHANNA\n",
    "Author-email: maggie.mhanna@renault.com\n",
    "License: Apache\n",
    "Description: UNKNOWN\n",
    "Platform: UNKNOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bikesharingmodel/setup.cfg\n",
    "[egg_info]\n",
    "tag_build = \n",
    "tag_date = 0\n",
    "tag_svn_revision = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bikesharingmodel/setup.py\n",
    "from setuptools import find_packages, setup\n",
    "\n",
    "NAME = 'trainer'\n",
    "VERSION = '0.1'\n",
    "AUTHOR = 'MA MHANNA'\n",
    "EMAIL = 'maggie.mhanna@renault.com'\n",
    "\n",
    "REQUIRED_PACKAGES = ['tensorflow==2.2.0', \n",
    "                     'tensorflow-cpu==2.2.0',\n",
    "                     'tensorflow_transform==0.22.0',\n",
    "                     'tensorflow_model_analysis==0.22.0', \n",
    "                     'apache_beam[gcp]==2.20.0', \n",
    "                     'pyarrow==0.16.0',\n",
    "                     'tfx-bsl==0.22.0',\n",
    "                     'absl-py==0.8.1']\n",
    "\n",
    "\n",
    "setup(\n",
    "    name=NAME,\n",
    "    version=VERSION,\n",
    "    author = AUTHOR,\n",
    "    author_email = EMAIL,\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='Bike Sharing Demand Prediction.',\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bikesharingmodel/trainer/model.py\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pkg_resources\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_model_analysis as tfma\n",
    "\n",
    "print('INFO: TF version -- {}'.format(tf.__version__))\n",
    "print('INFO: TFT version -- {}'.format(pkg_resources.get_distribution(\"tensorflow_transform\").version))\n",
    "print('INFO: TFMA version -- {}'.format(pkg_resources.get_distribution(\"tensorflow_model_analysis\").version))\n",
    "print('INFO: Beam version -- {}'.format(pkg_resources.get_distribution(\"apache_beam\").version))\n",
    "print('INFO: Pyarrow version -- {}'.format(pkg_resources.get_distribution(\"pyarrow\").version))\n",
    "print('INFO: tfx-bsl version -- {}'.format(pkg_resources.get_distribution(\"tfx-bsl\").version))\n",
    "print('INFO: absl-py version -- {}'.format(pkg_resources.get_distribution(\"absl-py\").version))\n",
    "\n",
    "# Features, labels, and key columns\n",
    "NUMERIC_FEATURE_KEYS=[\"temp\", \"atemp\", \"humidity\", \"windspeed\"] \n",
    "CATEGORICAL_FEATURE_KEYS=[\"season\", \"weather\", \"daytype\"] \n",
    "KEY_COLUMN = \"datetime\"\n",
    "LABEL_COLUMN = \"count\"\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key \n",
    "    \n",
    "def _get_session_config_from_env_var():\n",
    "    \"\"\"Returns a tf.ConfigProto instance that has appropriate device_filters\n",
    "    set.\"\"\"\n",
    "\n",
    "    tf_config = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "\n",
    "    # Master should only communicate with itself and ps\n",
    "    if (tf_config and 'task' in tf_config and 'type' in tf_config[\n",
    "            'task'] and 'index' in tf_config['task']):\n",
    "        if tf_config['task']['type'] == 'master':\n",
    "            return tf.ConfigProto(device_filters=['/job:ps', '/job:master'])\n",
    "        # Worker should only communicate with itself and ps\n",
    "        elif tf_config['task']['type'] == 'worker':\n",
    "            return tf.ConfigProto(device_filters=[\n",
    "                '/job:ps',\n",
    "                '/job:worker/task:%d' % tf_config['task']['index']\n",
    "            ])\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_dataset_size(file_path):\n",
    "    \"\"\"Function that fetchs the size of the Tfrecords dataset.\"\"\"\n",
    "    size = 1\n",
    "    file_list = tf.io.gfile.glob(file_path)\n",
    "    for file in file_list:\n",
    "        for record in tf.compat.v1.io.tf_record_iterator(file, options=tf.io.TFRecordOptions(\n",
    "    compression_type='GZIP')):\n",
    "            size += 1\n",
    "    return size\n",
    "    \n",
    "# Train and Evaluate input functions\n",
    "    \n",
    "def input_fn(data_path, label_column, tf_transform_output, batch_size, mode = tf.estimator.ModeKeys.TRAIN):\n",
    "    \"\"\"Create an input function reading TFRecord files using the data API.\n",
    "    Args:\n",
    "        data_path: path of the data in tfrecords format\n",
    "        mode: tf estimator mode key\n",
    "        batch_size: number of observations in batch\n",
    "\n",
    "    Returns:\n",
    "        input_fn: data input function\n",
    "    \"\"\"\n",
    "    \n",
    "    features_spec = tf_transform_output.transformed_feature_spec()\n",
    "\n",
    "    def _input_fn():\n",
    "        # Create list of files in the data path\n",
    "        file_list = tf.io.gfile.glob(data_path)\n",
    "\n",
    "        # Create dataset from file list\n",
    "        dataset = tf.data.TFRecordDataset(filenames=file_list, compression_type = \"GZIP\", num_parallel_reads=5)\n",
    "        def parse_example(example):\n",
    "            parsed_features = tf.io.parse_single_example(example, features_spec)\n",
    "            label = parsed_features.pop(label_column)\n",
    "            return parsed_features, label\n",
    "          \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely, we'll set this in train spec\n",
    "            dataset = dataset.shuffle(buffer_size=10*batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after one epoch\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.map(parse_example, num_parallel_calls=5)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    \n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "def create_feature_columns(tf_transform_output):\n",
    "    \n",
    "    numeric_columns = [\n",
    "      tf.feature_column.numeric_column(transformed_name(key))\n",
    "      for key in NUMERIC_FEATURE_KEYS\n",
    "    ]\n",
    "    \n",
    "    categorical_columns = [\n",
    "      tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "        transformed_name(key), \n",
    "        vocabulary_file=tf_transform_output.vocabulary_file_by_name(\n",
    "            vocab_filename=key), \n",
    "        dtype=tf.dtypes.string,\n",
    "        default_value=None, \n",
    "        num_oov_buckets=0)\n",
    "      for key in CATEGORICAL_FEATURE_KEYS\n",
    "    ]\n",
    "    \n",
    "    indicator_columns = [\n",
    "      tf.feature_column.indicator_column(categorical_column)\n",
    "      for categorical_column in categorical_columns\n",
    "    ]\n",
    "       \n",
    "    feature_columns = numeric_columns + indicator_columns\n",
    "\n",
    "    return feature_columns\n",
    "\n",
    "\n",
    "def rmse(labels, predictions):\n",
    "    pred_values = predictions['predictions']\n",
    "    rmse = tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    rmse.update_state(y_true=labels, y_pred=pred_values)\n",
    "    return {'rmse': rmse}\n",
    "\n",
    "def rmse_2(labels, predictions):\n",
    "    pred_values = predictions['predictions']\n",
    "    rmse = tf.compat.v1.metrics.root_mean_squared_error(labels, pred_values)\n",
    "    return {'rmse': rmse}\n",
    "\n",
    "\n",
    "def mae(labels, predictions):\n",
    "    pred_values = tf.squeeze(input = predictions[\"predictions\"], axis = -1)\n",
    "    mae = tf.keras.metrics.MeanAbsoluteError(name=\"mae\")\n",
    "    mae.update_state(y_true=labels, y_pred=pred_values)\n",
    "    return {'mae': mae}\n",
    "\n",
    "\n",
    "def create_estimator_model(output_dir, feature_columns, hidden_units, run_config):\n",
    "    model = tf.estimator.DNNRegressor(\n",
    "        model_dir = output_dir,\n",
    "        feature_columns = feature_columns,\n",
    "        hidden_units = hidden_units, # specify neural architecture\n",
    "        config = run_config\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def serving_input_fn(tf_transform_output, label_column):\n",
    "    \"\"\"Creates an input function reading from raw data.\n",
    "\n",
    "    Args:\n",
    "    tf_transform_output: Wrapper around output of tf.Transform.\n",
    "\n",
    "    Returns:\n",
    "    The serving input function.\n",
    "    \"\"\"\n",
    "    raw_feature_spec = tf_transform_output.raw_feature_spec()\n",
    "    # Remove label since it is not available during serving.\n",
    "    raw_feature_spec.pop(label_column)\n",
    "\n",
    "    def _input_fn():\n",
    "        \"\"\"Input function for serving.\"\"\"\n",
    "        # Get raw features by generating the basic serving input_fn and calling it.\n",
    "        # Here we generate an input_fn that expects a parsed Example proto to be fed\n",
    "        # to the model at serving time.  See also\n",
    "        # tf.estimator.export.build_raw_serving_input_receiver_fn.\n",
    "        raw_input_fn = tf.estimator.export.build_parsing_serving_input_receiver_fn(\n",
    "            raw_feature_spec, default_batch_size=None)\n",
    "        serving_input_receiver = raw_input_fn()\n",
    "\n",
    "        # Apply the transform function that was used to generate the materialized\n",
    "        # data.\n",
    "        raw_features = serving_input_receiver.features\n",
    "        transformed_features = tf_transform_output.transform_raw_features(\n",
    "            raw_features)\n",
    "\n",
    "        return tf.estimator.export.ServingInputReceiver(\n",
    "            transformed_features, serving_input_receiver.receiver_tensors)\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "def eval_input_receiver_fn(tf_transform_output, label_column):\n",
    "    \"\"\"Function that defines an input placeholder,\n",
    "     parses and returns features and labels for evaluation.\"\"\"\n",
    "    \n",
    "    def _input_fn():\n",
    "\n",
    "        serialized_tf_example = tf.compat.v1.placeholder(\n",
    "            dtype=tf.string, shape=[None], name='input_example_placeholder')\n",
    "\n",
    "        # This *must* be a dictionary containing a single key 'examples', which\n",
    "        # points to the input placeholder.\n",
    "        receiver_tensors = {'examples': serialized_tf_example}\n",
    "\n",
    "        transformed_feature_spec = tf_transform_output.transformed_feature_spec()\n",
    "             \n",
    "        features = tf.io.parse_example(serialized_tf_example, transformed_feature_spec)\n",
    "\n",
    "        return tfma.export.EvalInputReceiver(\n",
    "            features=features,\n",
    "            receiver_tensors=receiver_tensors,\n",
    "            labels=features[label_column])\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "# train and evaluate\n",
    "\n",
    "def train_and_evaluate(params):\n",
    "    \n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO) # so loss is printed during training\n",
    "\n",
    "    # Extract params from task.py\n",
    "    DATA_DIR = params[\"data_dir\"]\n",
    "    OUTPUT_DIR = params[\"output_dir\"]\n",
    "    HIDDEN_UNITS_1 = params[\"hidden_units_1\"]\n",
    "    HIDDEN_UNITS_2 = params[\"hidden_units_2\"]\n",
    "    HIDDEN_UNITS_3 = params[\"hidden_units_3\"]\n",
    "    BATCH_SIZE = params[\"batch_size\"]\n",
    "    NUM_EPOCHS = params[\"num_epochs\"]\n",
    "    LEARNING_RATE = params[\"learning_rate\"]\n",
    "\n",
    "    # Setting up paths \n",
    "    TRAIN_PATH = DATA_DIR+'/train*'\n",
    "    VAL_PATH = DATA_DIR+'/val*'\n",
    "    TEST_PATH = DATA_DIR+'/test*'\n",
    "    \n",
    "    # Training set size\n",
    "    TRAIN_SIZE = get_dataset_size(TRAIN_PATH)\n",
    "\n",
    "    NUM_STEPS = TRAIN_SIZE / BATCH_SIZE * NUM_EPOCHS # total steps for which to train model\n",
    "    CHECKPOINTS_STEPS = 16 # checkpoint every N steps\n",
    "    \n",
    "    tf_transform_output = tft.TFTransformOutput(os.path.join(DATA_DIR, 'tft_output'))\n",
    "\n",
    "    FEATURE_COLUMNS = create_feature_columns(tf_transform_output)\n",
    "    \n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        tf_random_seed = 1, # for reproducibility\n",
    "        save_checkpoints_steps = CHECKPOINTS_STEPS, # checkpoint every N steps\n",
    "        save_summary_steps = int(CHECKPOINTS_STEPS/5),\n",
    "        session_config = _get_session_config_from_env_var()\n",
    "        )\n",
    "    \n",
    "    estimator = create_estimator_model(OUTPUT_DIR, FEATURE_COLUMNS, [HIDDEN_UNITS_1, \n",
    "                                       HIDDEN_UNITS_2, HIDDEN_UNITS_3], run_config)\n",
    "    \n",
    "    estimator = tf.estimator.add_metrics(estimator = estimator, metric_fn = rmse) \n",
    "    estimator = tf.estimator.add_metrics(estimator = estimator, metric_fn = mae) \n",
    "\n",
    "    train_input_fn = input_fn(TRAIN_PATH, LABEL_COLUMN, tf_transform_output, \n",
    "                              BATCH_SIZE, tf.estimator.ModeKeys.TRAIN)\n",
    "    val_input_fn = input_fn(VAL_PATH, LABEL_COLUMN, tf_transform_output, \n",
    "                            BATCH_SIZE, tf.estimator.ModeKeys.EVAL)\n",
    "    \n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = train_input_fn,\n",
    "        max_steps = NUM_STEPS)\n",
    "\n",
    "    exporter = tf.estimator.LatestExporter(name = 'exporter', \n",
    "               serving_input_receiver_fn = serving_input_fn(tf_transform_output, LABEL_COLUMN))\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = val_input_fn,\n",
    "        steps = CHECKPOINTS_STEPS, # Number of steps to run evalution for at each checkpoint\n",
    "        start_delay_secs = 1, # wait at least N seconds before first evaluation (default 120)\n",
    "        throttle_secs = 16, # wait at least N seconds before each subsequent evaluation (default 600)\n",
    "        exporters = exporter) # export SavedModel once at the end of training\n",
    "    \n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator = estimator, \n",
    "        train_spec = train_spec, \n",
    "        eval_spec = eval_spec) \n",
    "    \n",
    "    # Also export the EvalSavedModel\n",
    "    tfma.export.export_eval_savedmodel(\n",
    "        estimator=estimator, export_dir_base=OUTPUT_DIR + '/eval_saved_model/',\n",
    "        eval_input_receiver_fn=eval_input_receiver_fn(tf_transform_output, LABEL_COLUMN))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile bikesharingmodel/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "from . import model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--data_dir\",\n",
    "        help = \"Location directory to read datasets\",\n",
    "        type = str,\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        help = \"Location directory to write checkpoints and export models\",\n",
    "        type = str,\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_units_1\",\n",
    "        help = \"Hidden layer 1 sizes to use for DNN feature columns\",\n",
    "        type = int,\n",
    "        default = 16\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_units_2\",\n",
    "        help = \"Hidden layer 2 sizes to use for DNN feature columns\",\n",
    "        type = int,\n",
    "        default = 16\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hidden_units_3\",\n",
    "        help = \"Hidden layer 3 sizes to use for DNN feature columns\",\n",
    "        type = int,\n",
    "        default = 16\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        help = \"Mini-batch gradient descent batch size\",\n",
    "        type = int,\n",
    "        default = 16\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_epochs\",\n",
    "        help = \"Number of epochs\",\n",
    "        type = int,\n",
    "        default = 10\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        help = \"Hidden layer 1 sizes to use for DNN feature columns\",\n",
    "        type = int,\n",
    "        default = 0.001\n",
    "    )\n",
    "    args = parser.parse_args().__dict__\n",
    "\n",
    "\n",
    "    # Append trial_id to path so trials don't overwrite each other\n",
    "    args[\"output_dir\"] = os.path.join(\n",
    "        args[\"output_dir\"],\n",
    "        json.loads(\n",
    "            os.environ.get(\"TF_CONFIG\", \"{}\")\n",
    "        ).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    ) \n",
    "    \n",
    "    # Run the training job\n",
    "    model.train_and_evaluate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"DirectRunner\" ]; then\n",
    "\n",
    "    # Start fresh each time\n",
    "    if ! [ \"$HANDLER\" = \"gs://\" ]; then\n",
    "        rm -R ${OUTPUT_DIR}\n",
    "    else\n",
    "        gsutil -m rm -R ${OUTPUT_DIR}\n",
    "    fi  \n",
    "\n",
    "    python -m bikesharingmodel.trainer.task \\\n",
    "        --data_dir=${DATA_DIR} \\\n",
    "        --output_dir=${OUTPUT_DIR}\n",
    "else \n",
    "echo \"RUNNER = \" $RUNNER\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running locally using gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"DirectRunner\" ]; then\n",
    "\n",
    "    # Start fresh each time\n",
    "    if ! [ \"$HANDLER\" = \"gs://\" ]; then\n",
    "        rm -R ${OUTPUT_DIR}\n",
    "    else\n",
    "        gsutil -m rm -R ${OUTPUT_DIR}\n",
    "    fi \n",
    "\n",
    "    # Use AI Platform to train the model in local file system\n",
    "    gcloud ai-platform local train \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=bikesharingmodel/trainer \\\n",
    "        -- \\\n",
    "        --data_dir=${DATA_DIR} \\\n",
    "        --output_dir=${OUTPUT_DIR}\n",
    "else \n",
    "echo \"RUNNER = \" $RUNNER\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%tensorboard --logdir $OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on AI Platform using gcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile config.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: n1-standard-4\n",
    "  evaluatorType: n1-standard-4\n",
    "  evaluatorCount: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"AIplatformRunner\" ]; then\n",
    "\n",
    "    # Start fresh each time\n",
    "    if ! [ \"$HANDLER\" = \"gs://\" ]; then\n",
    "        rm -R ${OUTPUT_DIR}\n",
    "    else\n",
    "        gsutil -m rm -R ${OUTPUT_DIR}\n",
    "    fi \n",
    "\n",
    "    JOB_NAME=${PIPELINE_VERSION}_${MODEL_VERSION}\n",
    "    echo $OUTPUT_DIR $REGION $JOB_NAME\n",
    "\n",
    "    gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "       --region=$REGION \\\n",
    "       --module-name=trainer.task \\\n",
    "       --package-path=bikesharingmodel/trainer \\\n",
    "       --staging-bucket=gs://$BUCKET_STAGING \\\n",
    "       --config=config.yaml \\\n",
    "       --runtime-version=$TF_VERSION \\\n",
    "       --python-version=$PYTHON_VERSION \\\n",
    "       -- \\\n",
    "       --data_dir=$DATA_DIR \\\n",
    "       --output_dir=$OUTPUT_DIR \n",
    "else \n",
    "echo \"RUNNER = \" $RUNNER\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"AIplatformRunner\" ]; then\n",
    "    JOB_NAME=${PIPELINE_VERSION}_${MODEL_VERSION}\n",
    "    gcloud ai-platform jobs describe $JOB_NAME\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on AI Platform with Hyperparameter Tuning\n",
    "\n",
    "AI Platform HyperTune, powered by Google Vizier, uses Bayesian Optimization by default, but also supports Grid Search and Random Search.\n",
    "\n",
    "When tuning just a few hyperparameters (say less than 4), Grid Search and Random Search work well, but when tunining several hyperparameters and the search space is large Bayesian Optimization is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: n1-standard-4\n",
    "  evaluatorType: n1-standard-4\n",
    "  evaluatorCount: 1\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 2\n",
    "    hyperparameterMetricTag: rmse\n",
    "    enableTrialEarlyStopping: True   \n",
    "    params:\n",
    "    - parameterName: hidden_units_1\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: hidden_units_2\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: hidden_units_3\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LOG_SCALE\n",
    "    - parameterName: batch_size\n",
    "      type: INTEGER\n",
    "      minValue: 8\n",
    "      maxValue: 64\n",
    "      scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"AIplatformRunner\" ]; then\n",
    "\n",
    "    # Start fresh each time\n",
    "    if ! [ \"$HANDLER\" = \"gs://\" ]; then\n",
    "        rm -R ${OUTPUT_DIR}\n",
    "    else\n",
    "        gsutil -m rm -R ${OUTPUT_DIR}\n",
    "    fi \n",
    "\n",
    "    JOB_NAME=${PIPELINE_VERSION}_${MODEL_VERSION}_hypertune\n",
    "    echo $OUTPUT_DIR $REGION $JOBNAME\n",
    "\n",
    "    gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "       --region=$REGION \\\n",
    "       --module-name=trainer.task \\\n",
    "       --package-path=bikesharingmodel/trainer \\\n",
    "       --staging-bucket=gs://$BUCKET_STAGING \\\n",
    "       --runtime-version=$TF_VERSION \\\n",
    "       --python-version=$PYTHON_VERSION \\\n",
    "       --config=hyperparam.yaml \\\n",
    "       -- \\\n",
    "       --data_dir=$DATA_DIR \\\n",
    "       --output_dir=$OUTPUT_DIR \\\n",
    "       --num_epochs=10  \n",
    "\n",
    "else \n",
    "echo \"RUNNER = \" $RUNNER\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$RUNNER\" = \"AIplatformRunner\" ]; then\n",
    "    JOB_NAME=${PIPELINE_VERSION}_${MODEL_VERSION}_hypertune\n",
    "    gcloud ai-platform jobs describe $JOB_NAME\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameter trial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "JOB_NAME=${PIPELINE_VERSION}_${MODEL_VERSION}_hypertune\n",
    "TRIAL_ID=$(gcloud ai-platform jobs describe $JOB_NAME --format 'value(trainingOutput.trials.trialId.slice(0))')\n",
    "echo $TRIAL_ID"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
