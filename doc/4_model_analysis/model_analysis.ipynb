{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Analysis and Validation\n",
    "\n",
    "The model analysis component performs deep analysis of the training results and helps you validate your exported models, ensuring that they are \"good enough\" to be pushed to production.\n",
    "\n",
    "This evaluation step helps guarantee that the model is promoted for serving only if it satisfies the quality criteria. The criteria include improved performance compared to previous models and fair performance on various data subsets. The output of this step is a set of performance metrics (our evaluation metric F1score for all parts/operations) and a decision on whether to promote the model to production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Libraries & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow==2.2.0\n",
      "tensorflow_model_analysis==0.22.0\n",
      "apache_beam[gcp]==2.20.0\n",
      "pyarrow==0.16.0\n",
      "tfx-bsl==0.22.0\n",
      "google-cloud-storage==1.28.0"
     ]
    }
   ],
   "source": [
    "!cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFMA\n",
    "\n",
    "When the model is exported after the training step, it must be evaluated on the test dataset to assess the model quality before deciding whether the model should be deployed. \n",
    "\n",
    "TensorFlow Model Analysis (TFMA), is a library for evaluating TensorFlow models. It allows us to evaluate our models in a distributed manner, using the same metrics defined in our trainer. These metrics can be computed over different slices/segments (countries, engine types, symptoms, error codes, â€¦) of data and visualized. We also track model performance over time so that we can be aware of and react to changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable TFMA visualization in Jupyter Notebook: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If running in a local Jupyter notebook, then these Jupyter extensions must be installed in the environment before running Jupyter. \n",
    "\n",
    "We don't need to install the Jupyter extensions inside the Kubeflow model analysis component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If Jupyter notebook is already installed in our home directory, add --user to these commands. If Jupyter is installed as root, or using a virtual environment, the parameter --sys-prefix might be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension --user\n",
    "#!jupyter nbextension install --py --symlink tensorflow_model_analysis --user\n",
    "#!jupyter nbextension enable tensorflow_model_analysis --user --py "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Librarires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.2.0\n",
      "TFMA version: 0.22.0\n",
      "INFO: Beam version -- 2.20.0\n",
      "INFO: Pyarrow version -- 0.16.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "import apache_beam as beam\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from prettytable import PrettyTable\n",
    "from ipywidgets.embed import embed_data\n",
    "from io import BytesIO\n",
    "\n",
    "import pkg_resources\n",
    "from google.cloud import storage\n",
    "\n",
    "print('TF version: {}'.format(tf.__version__))\n",
    "print('TFMA version: {}'.format(pkg_resources.get_distribution(\"tensorflow_model_analysis\").version))\n",
    "print('INFO: Beam version -- {}'.format(pkg_resources.get_distribution(\"apache_beam\").version))\n",
    "print('INFO: Pyarrow version -- {}'.format(pkg_resources.get_distribution(\"pyarrow\").version))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of input arguments for the model analysis component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"irn-70656-dev-1307100302\"\n",
    "REGION = 'europe-west1'\n",
    "BUCKET = \"bike-sharing-pipeline-metadata\"\n",
    "PIPELINE_VERSION = \"v0_1\"\n",
    "DATA_VERSION = \"200909_154702\"\n",
    "MODEL_VERSION = \"200909_163139\"\n",
    "TRIAL_ID = None\n",
    "RUNNER = \"DirectRunner\" # DirectRunner or DataflowRunner to run on Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Paths "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up some globals for the gcs files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some globals for gcs file\n",
    "HANDLER = 'gs://' # ../ for local data, gs:// for cloud data\n",
    "\n",
    "BASE_DIR = os.path.join(HANDLER, BUCKET, PIPELINE_VERSION)\n",
    "RUN_DIR = os.path.join(BASE_DIR, 'run', DATA_VERSION)\n",
    "DATA_DIR = os.path.join(RUN_DIR, 'data_transform')\n",
    "MODEL_DIR = os.path.join(RUN_DIR, 'model_training', MODEL_VERSION, str(TRIAL_ID) if TRIAL_ID is not None else \"\")\n",
    "OUTPUT_DIR = os.path.join(RUN_DIR, 'model_analysis', MODEL_VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some globals for gcs file\n",
    "HANDLER = 'gs://' # ../ for local data, gs:// for cloud data\n",
    "\n",
    "BASE_DIR = HANDLER + BUCKET+'/'+PIPELINE_VERSION\n",
    "RUN_DIR = BASE_DIR+'/run/'+DATA_VERSION\n",
    "DATA_DIR = RUN_DIR+'/data_transform'\n",
    "MODEL_DIR = RUN_DIR+'/model_training/' + MODEL_VERSION + ('/' + str(TRIAL_ID) if TRIAL_ID is not None else \"\") \n",
    "OUTPUT_DIR = RUN_DIR+'/model_analysis/' + MODEL_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = DATA_DIR+'/test*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features, labels, and key columns\n",
    "NUMERIC_FEATURE_KEYS=[\"temp\", \"atemp\", \"humidity\", \"windspeed\"] \n",
    "CATEGORICAL_FEATURE_KEYS=[\"season\", \"weather\", \"daytype\"] \n",
    "KEY_COLUMN = \"datetime\"\n",
    "LABEL_COLUMN = \"count\"\n",
    "\n",
    "def transformed_name(key):\n",
    "    return key "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dirs(path, pattern):\n",
    "    \"\"\"Function that returns all files in GCS directory corresponding to some pattern.\"\"\"\n",
    "\n",
    "    runs = tf.io.gfile.listdir(path)\n",
    "    runs = [re.sub('/', '', run) for run in runs]\n",
    "    runs = [re.match(pattern, run).group(0)\n",
    "            for run in runs if re.match(pattern, run) != None]\n",
    "    runs.sort()\n",
    "\n",
    "    return runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- Evaluation slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1- Specify model to use for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can specify the EvalSavedModel we saved previously in order to use it for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify model to use for evaluation\n",
    "eval_saved_model_path = MODEL_DIR+'/eval_saved_model/'\n",
    "eval_saved_model_path = eval_saved_model_path + list_dirs(eval_saved_model_path, '(\\d)+')[-1]\n",
    "\n",
    "eval_shared_model = tfma.default_eval_shared_model(eval_saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_gcs_dir(bucket_name, source_dir, destination_dir, mute=False):\n",
    "    \"\"\"Copies a GCS directory with all its blobs.\n",
    "    \n",
    "    Args:\n",
    "        bucket_name (string): bucket name\n",
    "        source_dir: path to directory containing the blobs to copy (without gs://{bucket_name}/)\n",
    "        destination_dir: path directory where to copy blobs (without gs://{bucket_name}/)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create storage client and set bucket\n",
    "    storage_client = storage.Client()\n",
    "    storage_bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # List all blobs in source directory\n",
    "    generic_source_dir = source_dir.replace('gs://'+bucket_name+'/', '')\n",
    "    generic_destination_dir = destination_dir.replace('gs://'+bucket_name+'/', '')\n",
    "\n",
    "    blobs = storage_client.list_blobs(bucket_name, prefix=generic_source_dir)\n",
    "\n",
    "    # Copy all blobs from source_dir to destination_dir\n",
    "    for blob in blobs:\n",
    "\n",
    "        # Define path inside destination directory\n",
    "        short_destination = blob.name.replace(generic_source_dir, '')\n",
    "\n",
    "        # Copy blob\n",
    "        blob_copy = storage_bucket.copy_blob(\n",
    "            blob, storage_bucket, generic_destination_dir+'/'+short_destination)\n",
    "\n",
    "        if not mute:\n",
    "            print(\"INFO: Blob {} in bucket {} copied to blob {} in bucket {}.\\n\".format(\n",
    "                blob.name, bucket_name, blob_copy.name, bucket_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Blob v0_1/run/200909_154702/model_training/200909_163139/eval_saved_model/1599669181/ in bucket bike-sharing-pipeline-metadata copied to blob v0_1/run/200909_154702/model_analysis/200909_163139/eval_saved_model/ in bucket bike-sharing-pipeline-metadata.\n",
      "\n",
      "INFO: Blob v0_1/run/200909_154702/model_training/200909_163139/eval_saved_model/1599669181/assets/ in bucket bike-sharing-pipeline-metadata copied to blob v0_1/run/200909_154702/model_analysis/200909_163139/eval_saved_model/assets/ in bucket bike-sharing-pipeline-metadata.\n",
      "\n",
      "INFO: Blob v0_1/run/200909_154702/model_training/200909_163139/eval_saved_model/1599669181/assets/daytype in bucket bike-sharing-pipeline-metadata copied to blob v0_1/run/200909_154702/model_analysis/200909_163139/eval_saved_model/assets/daytype in bucket bike-sharing-pipeline-metadata.\n",
      "\n",
      "INFO: Blob v0_1/run/200909_154702/model_training/200909_163139/eval_saved_model/1599669181/assets/season in bucket bike-sharing-pipeline-metadata copied to blob v0_1/run/200909_154702/model_analysis/200909_163139/eval_saved_model/assets/season in bucket bike-sharing-pipeline-metadata.\n",
      "\n",
      "INFO: Blob v0_1/run/200909_154702/model_training/200909_163139/eval_saved_model/1599669181/assets/weather in bucket bike-sharing-pipeline-metadata copied to blob v0_1/run/200909_154702/model_analysis/200909_163139/eval_saved_model/assets/weather in bucket bike-sharing-pipeline-metadata.\n",
      "\n",
      "INFO: Blob v0_1/run/200909_154702/model_training/200909_163139/eval_saved_model/1599669181/saved_model.pb in bucket bike-sharing-pipeline-metadata copied to blob v0_1/run/200909_154702/model_analysis/200909_163139/eval_saved_model/saved_model.pb in bucket bike-sharing-pipeline-metadata.\n",
      "\n",
      "INFO: Blob v0_1/run/200909_154702/model_training/200909_163139/eval_saved_model/1599669181/variables/ in bucket bike-sharing-pipeline-metadata copied to blob v0_1/run/200909_154702/model_analysis/200909_163139/eval_saved_model/variables/ in bucket bike-sharing-pipeline-metadata.\n",
      "\n",
      "INFO: Blob v0_1/run/200909_154702/model_training/200909_163139/eval_saved_model/1599669181/variables/variables.data-00000-of-00001 in bucket bike-sharing-pipeline-metadata copied to blob v0_1/run/200909_154702/model_analysis/200909_163139/eval_saved_model/variables/variables.data-00000-of-00001 in bucket bike-sharing-pipeline-metadata.\n",
      "\n",
      "INFO: Blob v0_1/run/200909_154702/model_training/200909_163139/eval_saved_model/1599669181/variables/variables.index in bucket bike-sharing-pipeline-metadata copied to blob v0_1/run/200909_154702/model_analysis/200909_163139/eval_saved_model/variables/variables.index in bucket bike-sharing-pipeline-metadata.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# copy eval_saved_model to output_dir\n",
    "copy_gcs_dir(BUCKET, eval_saved_model_path+'/', OUTPUT_DIR+'/eval_saved_model', mute=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2- Defining slices of evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the slice you want to visualize you create a `tfma.slicer.SingleSliceSpec`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a whole list of SliceSpecs, which will allow us to select any of the slices in the list. The list includes all the catagorical variables as well as variable length catagorical variables. With TFMA, we can also create feature crosses to analyze combinations of features. Let's add to the list two SliceSpec to look at a cross of `engineType` and `engineIndex`, as well as at a cross of `gearboxType` and `gearboxIndex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_slices = [[x] for x in CATEGORICAL_FEATURE_KEYS]\n",
    "\n",
    "# Defining slices of evaluation\n",
    "# An empty spec is required for the 'Overall' slice \n",
    "slices = [tfma.slicer.SingleSliceSpec()] \\\n",
    "       + [tfma.slicer.SingleSliceSpec(columns=x) for x in feature_slices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3- Write tfma evaluation results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: using run_model_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `tfma.run_model_analysis` for evaluation. Since this uses Beam's local runner, it's mainly for local, small-scale experimentation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tfma.run_model_analysis` requires an EvalSavedModel, and a list SliceSpecs.\n",
    "\n",
    "It will create an EvalResult, and use it to create a SlicingMetricsViewer using tfma.view.render_slicing_metrics, which will render a visualization of our dataset using the slice we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes your data is a TFRecords file containing records in the format\n",
    "# your model is expecting, e.g. tf.train.Example if you're using\n",
    "# tf.parse_example in your model.\n",
    "\n",
    "#eval_dir = OUTPUT_DIR+ \\\n",
    "#    '/eval_result'\n",
    "\n",
    "#eval_result = tfma.run_model_analysis(\n",
    "#    eval_shared_model=eval_shared_model,\n",
    "#    data_location=TEST_PATH,\n",
    "#    file_format='tfrecords',\n",
    "#    slice_spec=slices,\n",
    "#    output_path=eval_dir\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Apache Beam pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Beam is required to run distributed analysis. By default, Apache Beam runs in local mode but can also run in distributed mode using **Google Cloud Dataflow**.\n",
    "\n",
    "For **distributed evaluation**, construct an Apache Beam pipeline using a distributed runner. In the pipeline, use the tfma.ExtractEvaluateAndWriteResults for evaluation and to write out the results. The results can be loaded for visualization using tfma.load_eval_result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: TFMA Evaluation Job exported to gs://bike-sharing-pipeline-metadata/v0_1/run/200909_154702/model_analysis/200909_163139/eval_result\n",
      "INFO: TFMA Evaluation Results loaded from EvalResult(slicing_metrics=[((), {'': {'': {'rmse': {'doubleValue': 339.1432800292969}, 'average_loss': {'doubleValue': 115018.15625}, 'label/mean': {'doubleValue': 260.27410888671875}, 'post_export_metrics/example_count': {'doubleValue': 2178.0}, 'mae': {'doubleValue': 247.89231872558594}, 'prediction/mean': {'doubleValue': 0.11727461218833923}}}}), ((('season', 'fall'),), {'': {'': {'average_loss': {'doubleValue': 139115.953125}, 'label/mean': {'doubleValue': 289.0715026855469}, 'post_export_metrics/example_count': {'doubleValue': 811.0}, 'mae': {'doubleValue': 288.7800598144531}, 'prediction/mean': {'doubleValue': 0.2914578914642334}, 'rmse': {'doubleValue': 372.9825134277344}}}}), ((('weather', 'clear'),), {'': {'': {'prediction/mean': {'doubleValue': 0.13379088044166565}, 'rmse': {'doubleValue': 360.08135986328125}, 'average_loss': {'doubleValue': 129658.578125}, 'label/mean': {'doubleValue': 279.9909973144531}, 'post_export_metrics/example_count': {'doubleValue': 1334.0}, 'mae': {'doubleValue': 268.75946044921875}}}}), ((('daytype', 'weekend'),), {'': {'': {'rmse': {'doubleValue': 319.9737243652344}, 'average_loss': {'doubleValue': 102383.1796875}, 'label/mean': {'doubleValue': 247.57386779785156}, 'post_export_metrics/example_count': {'doubleValue': 643.0}, 'mae': {'doubleValue': 247.49557495117188}, 'prediction/mean': {'doubleValue': 0.0783107653260231}}}}), ((('weather', 'light rain'),), {'': {'': {'post_export_metrics/example_count': {'doubleValue': 152.0}, 'mae': {'doubleValue': 194.1734161376953}, 'prediction/mean': {'doubleValue': 0.10290505737066269}, 'rmse': {'doubleValue': 262.9820556640625}, 'average_loss': {'doubleValue': 69159.5546875}, 'label/mean': {'doubleValue': 194.2763214111328}}}}), ((('weather', 'cloudy'),), {'': {'': {'average_loss': {'doubleValue': 96868.2109375}, 'label/mean': {'doubleValue': 236.76156616210938}, 'post_export_metrics/example_count': {'doubleValue': 692.0}, 'mae': {'doubleValue': 236.67295837402344}, 'prediction/mean': {'doubleValue': 0.08859177678823471}, 'rmse': {'doubleValue': 311.236572265625}}}}), ((('daytype', 'workingday'),), {'': {'': {'post_export_metrics/example_count': {'doubleValue': 1463.0}, 'mae': {'doubleValue': 256.2756652832031}, 'prediction/mean': {'doubleValue': 0.13591940701007843}, 'rmse': {'doubleValue': 348.56207275390625}, 'average_loss': {'doubleValue': 121495.515625}, 'label/mean': {'doubleValue': 266.51129150390625}}}}), ((('daytype', 'holiday'),), {'': {'': {'label/mean': {'doubleValue': 246.9583282470703}, 'post_export_metrics/example_count': {'doubleValue': 72.0}, 'mae': {'doubleValue': 246.87196350097656}, 'prediction/mean': {'doubleValue': 0.08639175444841385}, 'rmse': {'doubleValue': 310.2247009277344}, 'average_loss': {'doubleValue': 96239.359375}}}}), ((('season', 'winter'),), {'': {'': {'mae': {'doubleValue': 234.42160034179688}, 'prediction/mean': {'doubleValue': 0.013936903327703476}, 'rmse': {'doubleValue': 317.3667907714844}, 'average_loss': {'doubleValue': 100721.671875}, 'label/mean': {'doubleValue': 243.18946838378906}, 'post_export_metrics/example_count': {'doubleValue': 1367.0}}}})], plots=[], config=model_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"season\"\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"weather\"\n",
      "}\n",
      "slicing_specs {\n",
      "  feature_keys: \"daytype\"\n",
      "}\n",
      "options {\n",
      "  compute_confidence_intervals {\n",
      "  }\n",
      "  min_slice_size {\n",
      "    value: 1\n",
      "  }\n",
      "}\n",
      ", data_location='<user provided PCollection>', file_format='<unknown>', model_location='gs://bike-sharing-pipeline-metadata/v0_1/run/200909_154702/model_training/200909_163139/eval_saved_model/1599669181')\n"
     ]
    }
   ],
   "source": [
    "eval_dir = OUTPUT_DIR+ \\\n",
    "    '/eval_result'\n",
    "\n",
    "with beam.Pipeline(runner='DirectRunner') as p:\n",
    "    _ = (p\n",
    "        # You can change the source as appropriate, e.g. read from BigQuery.\n",
    "        | 'ReadData' >> beam.io.ReadFromTFRecord(TEST_PATH)\n",
    "        | 'ExtractEvaluateAndWriteResults' >>\n",
    "        tfma.ExtractEvaluateAndWriteResults(\n",
    "            eval_shared_model=eval_shared_model,\n",
    "            output_path=eval_dir,\n",
    "            compute_confidence_intervals=False,\n",
    "            slice_spec=slices\n",
    "        ))\n",
    "\n",
    "print(\"INFO: TFMA Evaluation Job exported to {}\".format(eval_dir))\n",
    "\n",
    "# Load slices evaluation results\n",
    "eval_result = tfma.load_eval_result(eval_dir)\n",
    "print(\"INFO: TFMA Evaluation Results loaded from {}\".format(eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our EvalResult, we can use it to create a SlicingMetricsViewer using `tfma.view.render_slicing_metrics`, which will render a visualization of our dataset using the slices we created.\n",
    "\n",
    "To show the overall evaluation metrics viewer, we can simply use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c887b6e30e6b45be9c753b13fbfc1057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'post_export_metrics/example_count'}, data=[{'slice': 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use tfma.view.render_slicing_metrics over a specified slice, we can either use the name of the column (by setting slicing_column) or provide a tfma.slicer.SingleSliceSpec (by setting slicing_spec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1dba1891d974aefb83cd24f29cfa24b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'post_export_metrics/example_count'}, data=[{'slice': 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result, slicing_column='season')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ad9247624547e9957346ae90b7617f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SlicingMetricsViewer(config={'weightedExamplesColumn': 'post_export_metrics/example_count'}, data=[{'slice': 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfma.view.render_slicing_metrics(eval_result, slicing_spec=tfma.slicer.SingleSliceSpec(columns=['weather']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Note that widgets are not visualized correctly in JupyterLab, But we will be able to see them later using HTML</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Tracking Model Performance Over Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training dataset will be used for training our model, and will hopefully be representative of our test dataset and the inference data that will be sent to our model in production. However, in many cases the distribution of the new data will start to change enough so that the performance of our model will start to decay with time.\n",
    "\n",
    "That means that we need to monitor and measure our model's performance on an ongoing basis, so that you can be aware of and react to changes.\n",
    "\n",
    "Model drift can occur when there is some form of change to feature data or target dependencies. We can broadly classify these changes into the following three categories: concept drift, data drift, and upstream data changes.\n",
    "\n",
    "* **Concept Drift**: When statistical properties of the target variable change, the very concept of what you are trying to predict changes as well. \n",
    "\n",
    "* **Data Drift**: The features used to train a model are selected from the input data. When statistical properties of this input data change, it will have a downstream impact on the modelâ€™s quality. \n",
    "\n",
    "* **Upstream Data Changes**: Sometimes there can be operational changes in the data pipeline upstream which could have an impact on the model quality. For example, changes to feature encoding such as switching from Fahrenheit to Celsius \n",
    "\n",
    "Given that there will be such changes after a model is deployed to production, your best course of action is to monitor for changes and take action when changes occur. Having a feedback loop from a monitoring system, and refreshing models over time, will help avoid model staleness.\n",
    "\n",
    "![](./model_decay.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every pipeline run, the trained model is evaluated in the model analysis (overall evaluation and slices evaluation) component using TFMA and the results are saved to `eval_slices_result_dir`.\n",
    "\n",
    "At each pipeline run, the EvalResult is saved to the following directory:\n",
    "\n",
    "gs://[bucket]/[family_id]/[pipeline_version]/run/[preproc_version]/analysis/tfma/eval_slices_result/\n",
    "eval_slices_result__[pipeline_version]\\_\\_[preproc_version]\\_\\_[hypertune_version].\n",
    "\n",
    "It doesn't make sense to compare models with different pipeline versions, as these models could have completely different objectives, labels... We will be comparing only all models that has the same pipeline version of our current trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get list of all evaluation results EvalResult of previous evaluated models within the same pipeline version automatically from GCS directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Comparing with previous trial results.\n"
     ]
    }
   ],
   "source": [
    "print(\"INFO: Comparing with previous trial results.\")\n",
    "\n",
    "# Get list of all runs within the same pipeline version\n",
    "eval_dirs = []\n",
    "runs = list_dirs(BASE_DIR+'/run', '(\\d){6}_(\\d){6}')\n",
    "\n",
    "# Get list of all evaluation results (only best hypertune trials)\n",
    "for run in runs:\n",
    "    run_eval_dir = BASE_DIR+'/run/'+run+'/model_analysis/'\n",
    "    models = list_dirs(run_eval_dir, '(\\d){6}_(\\d){6}')\n",
    "    for model in models: \n",
    "        model_eval_dir = run_eval_dir + model + '/eval_result'\n",
    "        if tf.io.gfile.exists(model_eval_dir + '/eval_config.json'):\n",
    "            eval_dirs = eval_dirs + [model_eval_dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://bike-sharing-pipeline-metadata/v0_1/run/200909_154702/model_analysis/200909_163139/eval_result']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the list of historical evaluation results by providing their file paths to `tfma.load_eval_results`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best trial evaluation results\n",
    "eval_results = tfma.load_eval_results(\n",
    "    eval_dirs,\n",
    "    tfma.constants.MODEL_CENTRIC_MODE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show the overall time series of all provided evaluation results, we can simply use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c262736b184a04a4fdfc375b0ef4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TimeSeriesViewer(config={'isModelCentric': True}, data=[{'metrics': {'': {'': {'post_export_metrics/example_coâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfma.view.render_time_series(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Note that widgets are not visualized correctly in JupyterLab, But we will be able to see them when embedded into HTML</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- How does it look today?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the above approach is that we are comparing evaluation results of all our historical models in the same pipeline version already done over their corresponding old test datasets. Each model is evaluated over it's own set of test dataset that was created at the same time our old model is created. \n",
    "\n",
    "Instead, we need to compare our models on the same new test dataset, in order to compare how all models are doing today on recent data. However, for this particular use case, we have 2 main challenges to address in order to do that.\n",
    "* We need to compare deployed models only, a presence of an EvalSavedModel or EvalResult path doesn't necessarily mean that this particular model is deployed.\n",
    "* We need to adapt our new dataset to old models, removing target labels that were not taken into consideration with our old models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Measuring performance on new test data.\n"
     ]
    }
   ],
   "source": [
    "print(\"INFO: Measuring performance on new test data.\")\n",
    "\n",
    "def get_eval_result(eval_saved_model_dir, output_dir, data_dir, slice_spec):\n",
    "    \"\"\"Runs tfma model analysis locally to compute slicing metrics.\"\"\"\n",
    "\n",
    "    eval_shared_model = tfma.default_eval_shared_model(eval_saved_model_path=eval_saved_model_dir)\n",
    "\n",
    "    return tfma.run_model_analysis(\n",
    "        eval_shared_model=eval_shared_model,\n",
    "        data_location=data_dir,\n",
    "        file_format='tfrecords',\n",
    "        slice_spec=slice_spec,\n",
    "        output_path=output_dir,\n",
    "        extractors=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Tensorflow version (2.2.0) found. Note that TFMA support for TF 2.0 is currently in beta\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Comparing with previous trial results.\n"
     ]
    }
   ],
   "source": [
    "print(\"INFO: Comparing with previous trial results.\")\n",
    "\n",
    "eval_today_dirs = []\n",
    "eval_today_results_dict = {}\n",
    "\n",
    "# Get list of all runs within the same pipeline version\n",
    "runs = list_dirs(BASE_DIR+'/run', '(\\d){6}_(\\d){6}')\n",
    "\n",
    "# Get list of all evaluation results (only best hypertune trials)\n",
    "for run in runs:\n",
    "    \n",
    "    run_eval_saved_model_dir = BASE_DIR+'/run/'+run+'/model_analysis/'\n",
    "    models = list_dirs(run_eval_saved_model_dir, '(\\d){6}_(\\d){6}')\n",
    "    for model in models: \n",
    "        model_eval_saved_model_dir = run_eval_saved_model_dir + model + '/eval_saved_model'\n",
    "        if tf.io.gfile.exists(model_eval_saved_model_dir + '/saved_model.pb'):\n",
    "            \n",
    "            run_eval_today_dir = OUTPUT_DIR + '/eval_today_result/eval_result_{}_{}'.format(run, model)\n",
    "            eval_today_results_dict[run, model] = get_eval_result(model_eval_saved_model_dir, run_eval_today_dir, TEST_PATH, slices)               \n",
    "            eval_today_dirs = eval_today_dirs + [run_eval_today_dir]\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gs://bike-sharing-pipeline-metadata/v0_1/run/200909_154702/model_analysis/200909_163139/eval_today_result/eval_result_200909_154702_200909_163139']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_today_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results on new test data\n",
    "eval_today_results = tfma.load_eval_results(\n",
    "    eval_today_dirs,\n",
    "    tfma.constants.MODEL_CENTRIC_MODE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the time series for evaluations on new data as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8921fc52dd994f2db0186259f6b6ae97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TimeSeriesViewer(config={'isModelCentric': True}, data=[{'metrics': {'': {'': {'post_export_metrics/example_coâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfma.view.render_time_series(eval_today_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of EvalResult paths would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gs://bike-sharing-pipeline-metadata/v0_1/run/200909_154702/model_analysis/200909_163139/eval_today_result/eval_result_200909_154702_200909_163139']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(eval_today_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Write Evaluation Results to Artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The display functions of tfma render views as widgets in order to display them in Jupyter Notebook. It would be interesting if we can visualize these results in the Pipelines UI.\n",
    "\n",
    "The Kubeflow Pipelines UI offers built-in support for several types of visualizations, which we can use for this purpose. An output artifact is an output emitted by a pipeline component, which the Kubeflow Pipelines UI understands and can render as rich visualizations. \n",
    "\n",
    "Itâ€™s useful for pipeline components to include artifacts so that you can provide for performance evaluation, quick decision making for the run, or comparison across different runs. Artifacts also make it possible to understand how the pipelineâ€™s various components work. An artifact can range from a plain textual view of the data to rich interactive visualizations.\n",
    "\n",
    "To make use of this programmable UI, our pipeline component must write a JSON file to the componentâ€™s local filesystem. We can do this at any point during the pipeline execution.\n",
    "\n",
    "Available output viewers:\n",
    "* Confusion matrix \n",
    "* Markdown \n",
    "* ROC curve\n",
    "* Table\n",
    "* TensorBoard\n",
    "* Web app \n",
    "\n",
    "The web-app viewer provides flexibility for **rendering our custom tfma output**. We can specify an HTML file that our component creates, and the Kubeflow Pipelines UI renders that HTML in the output page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tfma.view.render_slicing_metrics` renders the slicing metrics view as widget.\n",
    "\n",
    "`tfma.view.render_time_series` renders the time series view as widget.\n",
    "\n",
    "To learn more about widgets, refer to the [following link](https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Basics.html).\n",
    "\n",
    "Jupyter notebook widgets support embedding the widget in a static HTML file that can be loaded outside of a Jupyter notebook. Jupyter interactive widgets can be serialized and embedded into **static web pages** using the module `ipywidgets.embed`. The `ipywidgets.embed` module provides several functions for embedding widgets into HTML documents programatically.\n",
    "\n",
    "We can use the function `embed_minimal_html` to create a simple, stand-alone HTML page. But for greater granularity than that afforded by `embed_minimal_html`; as we would like to control the structure of the HTML document in which the widgets are embedded; we can use `embed_data` to get JSON exports of specific parts of the widget state. \n",
    "\n",
    "For more information about how can this be done, refer to the [following link](https://ipywidgets.readthedocs.io/en/latest/embedding.html#python-interface)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html scripts for analysis artifacts\n",
    "_STATIC_HTML_TEMPLATE = \"\"\"\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Slicing Metrics</title>\n",
    "    <!-- Load RequireJS, used by the IPywidgets for dependency management -->\n",
    "    <script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js\"\n",
    "            integrity=\"sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=\"\n",
    "            crossorigin=\"anonymous\">\n",
    "    </script>\n",
    "    <!-- Load IPywidgets bundle for embedding. -->\n",
    "    <script src=\"https://unpkg.com/@jupyter-widgets/html-manager@^*/dist/embed-amd.js\"\n",
    "            crossorigin=\"anonymous\">\n",
    "    </script>\n",
    "    \n",
    "    <!-- Load vulcanized tfma code from prebuilt js file. -->\n",
    "    <script src=\"https://raw.githubusercontent.com/tensorflow/model-analysis/v0.22.0/tensorflow_model_analysis/static/vulcanized_tfma.js\">\n",
    "    </script> \n",
    "    <!-- Load IPywidgets bundle for embedding. -->\n",
    "    <script>\n",
    "      require.config({{\n",
    "        paths: {{\n",
    "          \"tfma_widget_js\": \"https://raw.githubusercontent.com/tensorflow/model-analysis/v0.22.0/tensorflow_model_analysis/static/index\", \n",
    "        }} \n",
    "      }});\n",
    "    </script>\n",
    "    <!-- The state of all the widget models on the page -->\n",
    "    <script type=\"application/vnd.jupyter.widget-state+json\">\n",
    "      {manager_state}\n",
    "    </script>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Slicing Metrics</h1>\n",
    "    {slicing_widget_views}\n",
    "    <h1>Comparison with Past Trial Results</h1>\n",
    "    {ts_widget_views}\n",
    "    <h1>How Does It Look Today</h1>\n",
    "    {ts_today_widget_views}\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "_SLICING_METRICS_WIDGET_TEMPLATE = \"\"\"\n",
    "    <div id=\"slicing-metrics-widget-{0}\">\n",
    "      <script type=\"application/vnd.jupyter.widget-view+json\">\n",
    "        {1}\n",
    "      </script>\n",
    "    </div>\n",
    "\"\"\"\n",
    "_TS_METRICS_WIDGET_TEMPLATE = \"\"\"\n",
    "    <div id=\"ts-metrics-widget-{0}\">\n",
    "      <script type=\"application/vnd.jupyter.widget-view+json\">\n",
    "        {1}\n",
    "      </script>\n",
    "    </div>\n",
    "\"\"\"\n",
    "_TS_TODAY_METRICS_WIDGET_TEMPLATE = \"\"\"\n",
    "    <div id=\"ts-today-metrics-widget-{0}\">\n",
    "      <script type=\"application/vnd.jupyter.widget-view+json\">\n",
    "        {1}\n",
    "      </script>\n",
    "    </div>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **embed_data** to get JSON exports of specific parts of the widget state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_static_html_output(eval_result, eval_results, eval_today_results,\n",
    "     slicing_specs, html_output_dir):\n",
    "    \"\"\"W R I T E  D O C S T R I N G!!\"\"\"\n",
    "\n",
    "    # Slicing Metrics Eval Result\n",
    "    if slicing_specs is not None:\n",
    "        slicing_metrics_views = [\n",
    "          tfma.view.render_slicing_metrics(\n",
    "              eval_result,\n",
    "              slicing_spec=slicing_spec)\n",
    "          for slicing_spec in slicing_specs\n",
    "        ]\n",
    "    else:\n",
    "        slicing_metrics_views = [\n",
    "          tfma.view.render_slicing_metrics(eval_result)\n",
    "        ]\n",
    "        \n",
    "    # Time series Eval Result\n",
    "    ts_metrics_view = tfma.view.render_time_series(\n",
    "            eval_results, display_full_path=False)\n",
    "    ts_today_metrics_view = tfma.view.render_time_series(\n",
    "            eval_today_results, display_full_path=False)\n",
    "        \n",
    "    slicing_data = embed_data(views=slicing_metrics_views)\n",
    "    manager_state = json.dumps(slicing_data['manager_state'])\n",
    "    slicing_widget_views = [json.dumps(view) for view in slicing_data['view_specs']]\n",
    "    slicing_views_html = \"\"\n",
    "    \n",
    "    for idx, view in enumerate(slicing_widget_views):\n",
    "        slicing_views_html += _SLICING_METRICS_WIDGET_TEMPLATE.format(idx, view)\n",
    "    \n",
    "    ts_data = embed_data(views=ts_metrics_view)\n",
    "    ts_today_data = embed_data(views=ts_today_metrics_view)\n",
    "    ts_widget_views = [json.dumps(view) for view in ts_data['view_specs']]\n",
    "    ts_today_widget_views = [json.dumps(view) for view in ts_today_data['view_specs']]\n",
    "    ts_views_html = \"\"\n",
    "    ts_today_views_html = \"\"\n",
    "\n",
    "    for idx, view in enumerate(ts_widget_views):\n",
    "        ts_views_html += _TS_METRICS_WIDGET_TEMPLATE.format(idx, view)\n",
    "    for idx, view in enumerate(ts_today_widget_views):\n",
    "        ts_today_views_html += _TS_TODAY_METRICS_WIDGET_TEMPLATE.format(idx, view)       \n",
    "    \n",
    "    rendered_template = _STATIC_HTML_TEMPLATE.format(\n",
    "        manager_state=manager_state,\n",
    "        slicing_widget_views=slicing_views_html,\n",
    "        ts_widget_views=ts_views_html,\n",
    "        ts_today_widget_views=ts_today_views_html)\n",
    "    \n",
    "    _OUTPUT_HTML_FILE = \"index.html\"\n",
    "\n",
    "    static_html_path = html_output_dir+'/'+_OUTPUT_HTML_FILE\n",
    "       \n",
    "    with open(os.path.join(\".\", _OUTPUT_HTML_FILE), \"wb\") as f:\n",
    "        f.write(rendered_template.encode('utf-8'))\n",
    "\n",
    "    print(\"INFO: Writing html artifacts to {}\".format(static_html_path))    \n",
    "    # upload file to gs\n",
    "    tf.io.gfile.copy(\n",
    "        _OUTPUT_HTML_FILE,\n",
    "        static_html_path,\n",
    "        overwrite=True\n",
    "    )   \n",
    "\n",
    "    metadata = {\n",
    "        'outputs' : [\n",
    "            {\n",
    "            'type': 'web-app',\n",
    "            'storage': 'gcs',\n",
    "            'source': static_html_path,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with file_io.FileIO('./mlpipeline-ui-metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Writing html artifacts to gs://bike-sharing-pipeline-metadata/v0_1/run/200909_154702/model_analysis/200909_163139/index.html\n",
      "INFO: HTML artifacts generated successfully.\n"
     ]
    }
   ],
   "source": [
    "generate_static_html_output(eval_result, eval_results, eval_today_results,\n",
    "     slices, OUTPUT_DIR)\n",
    "print(\"INFO: HTML artifacts generated successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OK or KO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('200909_154702', '200909_163139')])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_today_results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPerf = pd.DataFrame(columns=['pipeline_version', 'data_version', 'model_version', 'rmse'])\n",
    "\n",
    "for version in list(eval_today_results_dict.keys()):\n",
    "    dfPerf = dfPerf.append({'pipeline_version': PIPELINE_VERSION, \n",
    "                          'data_version': version[0], \n",
    "                          'model_version': version[1], \n",
    "                          'eval_data_version': DATA_VERSION,\n",
    "                          'rmse': eval_today_results_dict[version][0][0][1]['']['']['rmse']['doubleValue']}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pipeline_version</th>\n",
       "      <th>data_version</th>\n",
       "      <th>model_version</th>\n",
       "      <th>rmse</th>\n",
       "      <th>eval_data_version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>v0_1</td>\n",
       "      <td>200909_154702</td>\n",
       "      <td>200909_163139</td>\n",
       "      <td>339.14328</td>\n",
       "      <td>200909_154702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pipeline_version   data_version  model_version       rmse eval_data_version\n",
       "0             v0_1  200909_154702  200909_163139  339.14328     200909_154702"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dfPerf.loc[dfPerf.model_version == MODEL_VERSION, \"rmse\"].item() == min(dfPerf[\"rmse\"]):\n",
    "    deployment_flag = 'OK'\n",
    "else: \n",
    "    deployment_flag = 'KO'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "print(deployment_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPerf.to_csv(\"dfPerf.csv\", index=False)\n",
    "\n",
    "dfPerf_dir = OUTPUT_DIR + '/perf_models_today.csv'\n",
    "\n",
    "# upload file to gs\n",
    "tf.io.gfile.copy(\n",
    "    \"dfPerf.csv\",\n",
    "    dfPerf_dir,\n",
    "    overwrite=True\n",
    ")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPerf_schema = dfPerf.columns.to_list()\n",
    "\n",
    "metadata = {\n",
    "    'outputs' : [\n",
    "        {\n",
    "        'type': 'table',\n",
    "        'storage': 'gcs',\n",
    "        'format': 'csv',\n",
    "        'header': dfPerf_schema,\n",
    "        'source': dfPerf_dir\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with file_io.FileIO('mlpipeline-ui-metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m55",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m55"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
